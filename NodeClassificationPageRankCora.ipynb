{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zgNRxCONcpBh",
        "outputId": "5232f1e5-e6b7-4e48-b766-edf5291e19d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/repo.html\n",
            "Requirement already satisfied: dgl in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (1.13.0)\n",
            "Requirement already satisfied: networkx>=2.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (3.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (2.31.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (4.66.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\taneem\\appdata\\roaming\\python\\python312\\site-packages (from dgl) (5.9.8)\n",
            "Requirement already satisfied: torchdata>=0.5.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (0.8.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dgl) (2.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->dgl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->dgl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->dgl) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->dgl) (2024.2.2)\n",
            "Requirement already satisfied: torch>=2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchdata>=0.5.0->dgl) (2.3.0+cu121)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->dgl) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (1.12)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2->torchdata>=0.5.0->dgl) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata>=0.5.0->dgl) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2->torchdata>=0.5.0->dgl) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2->torchdata>=0.5.0->dgl) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=2->torchdata>=0.5.0->dgl) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: labml-nn in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.137)Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: labml==0.4.168 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.4.168)\n",
            "Requirement already satisfied: labml-helpers==0.4.89 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.4.89)\n",
            "Requirement already satisfied: torch in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchtext in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.18.0)\n",
            "Requirement already satisfied: torchvision in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.18.0)\n",
            "Requirement already satisfied: einops in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.8.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (1.26.4)\n",
            "Requirement already satisfied: fairscale in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml-nn) (0.4.13)\n",
            "Requirement already satisfied: gitpython in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml==0.4.168->labml-nn) (3.1.43)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from labml==0.4.168->labml-nn) (6.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (2024.3.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->labml-nn) (2021.4.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext->labml-nn) (4.66.4)\n",
            "Requirement already satisfied: requests in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchtext->labml-nn) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision->labml-nn) (10.2.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->labml-nn) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->labml-nn) (2021.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython->labml==0.4.168->labml-nn) (4.0.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->labml-nn) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext->labml-nn) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext->labml-nn) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext->labml-nn) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->torchtext->labml-nn) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->labml-nn) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->torchtext->labml-nn) (0.4.6)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython->labml==0.4.168->labml-nn) (5.0.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html\n",
        "%pip install labml-nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\taneem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic) (4.11.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9Fitk5OIy8Zw",
        "outputId": "6db5fcd9-cf07-4bdf-dafe-23609ea44851"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchdata\\datapipes\\__init__.py:18: UserWarning: \n",
            "################################################################################\n",
            "WARNING!\n",
            "The 'datapipes', 'dataloader2' modules are deprecated and will be removed in a\n",
            "future torchdata release! Please see https://github.com/pytorch/data/issues/1196\n",
            "to learn more and leave feedback.\n",
            "################################################################################\n",
            "\n",
            "  deprecation_warning()\n",
            "c:\\Users\\Taneem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import dgl.sparse as dglsp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LinearNeuralNetwork(nn.Module):\n",
        "    def __init__(self, nfeat, nclass, bias=True):\n",
        "        super(LinearNeuralNetwork, self).__init__()\n",
        "        self.W = nn.Linear(nfeat, nclass, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.W(x)\n",
        "\n",
        "\n",
        "def symmetric_normalize_adjacency(graph):\n",
        "    \"\"\"Symmetric normalize graph adjacency matrix.\"\"\"\n",
        "    indices = torch.stack(graph.edges())\n",
        "    n = graph.num_nodes()\n",
        "    adj = dglsp.spmatrix(indices, shape=(n, n))\n",
        "    deg_invsqrt = dglsp.diag(adj.sum(0)) ** -0.5\n",
        "    return deg_invsqrt @ adj @ deg_invsqrt\n",
        "\n",
        "\n",
        "def model_test(model, embeds):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(embeds)\n",
        "        pred = output.argmax(dim=-1)\n",
        "        test_mask, tv_mask = model.test_mask, model.tv_mask\n",
        "        loss_tv = F.mse_loss(output[tv_mask], model.label_one_hot[tv_mask])\n",
        "    accs = []\n",
        "    for mask in [tv_mask, test_mask]:\n",
        "        accs.append(\n",
        "            float((pred[mask] == model.label[mask]).sum() / mask.sum()))\n",
        "    return loss_tv.item(), accs[0], accs[1], pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "q_p4Vz1ac-v_",
        "outputId": "3050a0b8-2638-4d97-9ca6-b775f4b9e43c"
      },
      "outputs": [],
      "source": [
        "import dgl.sparse as dglsp\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class OGC(nn.Module):\n",
        "    def __init__(self, graph):\n",
        "        super(OGC, self).__init__()\n",
        "        self.linear_clf = LinearNeuralNetwork(\n",
        "            nfeat=graph.ndata[\"feat\"].shape[1],\n",
        "            nclass=graph.ndata[\"label\"].max().item() + 1,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        self.label = graph.ndata[\"label\"]\n",
        "        self.label_one_hot = F.one_hot(graph.ndata[\"label\"]).float()\n",
        "        # LIM trick, else use both train and val set to construct this matrix.\n",
        "        self.label_idx_mat = dglsp.diag(graph.ndata[\"train_mask\"]).float()\n",
        "\n",
        "        self.test_mask = graph.ndata[\"test_mask\"]\n",
        "        self.tv_mask = graph.ndata[\"train_mask\"] + graph.ndata[\"val_mask\"]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_clf(x)\n",
        "\n",
        "    def update_embeds(self, embeds, lazy_adj, args):\n",
        "        \"\"\"Update classifier's weight by training a linear supervised model.\"\"\"\n",
        "        pred_label = self(embeds).data\n",
        "        clf_weight = self.linear_clf.W.weight.data\n",
        "\n",
        "        # Update the smoothness loss via LGC.\n",
        "        embeds = dglsp.spmm(lazy_adj, embeds)\n",
        "\n",
        "        # Update the supervised loss via SEB.\n",
        "        deriv_sup = 2 * dglsp.matmul(\n",
        "            dglsp.spmm(self.label_idx_mat, -self.label_one_hot + pred_label),\n",
        "            clf_weight,\n",
        "        )\n",
        "        embeds = embeds - args.lr_sup * deriv_sup\n",
        "\n",
        "        args.lr_sup = args.lr_sup * args.decline\n",
        "        return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WVU5dTHRxtne",
        "outputId": "5580a423-927b-42b4-d027-fc62e391b398"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import time\n",
        "\n",
        "import dgl.sparse as dglsp\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from dgl import AddSelfLoop\n",
        "from dgl.data import CoraGraphDataset\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args([])\n",
        "args.dataset = \"cora\"\n",
        "args.decline = 0.9\n",
        "args.lr_sup = 0.001\n",
        "args.lr_clf = 0.5\n",
        "args.beta = 0.1\n",
        "args.max_sim_rate = 0.995\n",
        "args.max_patience = 2\n",
        "args.device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ScUDZ2Gjzw8z"
      },
      "outputs": [],
      "source": [
        "def train(model, embeds, lazy_adj, args):\n",
        "    patience = 0\n",
        "    _, _, last_acc, last_output = model_test(model, embeds)\n",
        "\n",
        "    tv_mask = model.tv_mask\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.lr_clf)\n",
        "\n",
        "    for i in range(64):\n",
        "        model.train()\n",
        "        output = model(embeds)\n",
        "        loss_tv = F.mse_loss(\n",
        "            output[tv_mask], model.label_one_hot[tv_mask], reduction=\"sum\"\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        loss_tv.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Updating node embeds by LGC and SEB jointly.\n",
        "        embeds = model.update_embeds(embeds, lazy_adj, args)\n",
        "\n",
        "        loss_tv, acc_tv, acc_test, pred = model_test(model, embeds)\n",
        "        print(\n",
        "            \"epoch {} loss_tv {:.4f} acc_tv {:.4f} acc_test {:.4f}\".format(\n",
        "                i + 1, loss_tv, acc_tv, acc_test\n",
        "            )\n",
        "        )\n",
        "\n",
        "        sim_rate = float(int((pred == last_output).sum()) / int(pred.shape[0]))\n",
        "        if sim_rate > args.max_sim_rate:\n",
        "            patience += 1\n",
        "            if patience > args.max_patience:\n",
        "                break\n",
        "        last_acc = acc_test\n",
        "        last_output = pred\n",
        "    return last_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Dec 31 19:02:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 546.33                 Driver Version: 546.33       CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650 Ti   WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   41C    P3              11W /  30W |    147MiB /  4096MiB |      5%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     11564    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "3bb63e36125742e782aca29a37a4574e",
            "83fd1cc79ffb4e358da3a4e2cdbf98dc",
            "056b821b52f549719e4b49583a1aad59",
            "62a93d8902634a0ca9f4dd35b1a7713a",
            "18c282ed6e7348c1bdeebc7dc031278a",
            "b2cd39f4ccda4efc8816a02337945955",
            "e43d5bbcbbe04b9ba7862febb04088d3",
            "69f198b8b1c0460985761f8477fac1ef",
            "f74db49c87c14c26b6045c893d1bfb64",
            "d1fd7c0036044bd096bbc87706177432",
            "8c3caa59addd4042be54ce85c0849b5f"
          ]
        },
        "id": "j02-1yeXIP8F",
        "outputId": "ace01551-9aed-48ee-dca2-f79203a81c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading C:\\Users\\Taneem\\.dgl\\cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Taneem\\.dgl\\cora_v2.zip: 100%|██████████| 132k/132k [00:02<00:00, 57.3kB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting file to C:\\Users\\Taneem\\.dgl\\cora_v2_d697a464\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n"
          ]
        }
      ],
      "source": [
        "transform = AddSelfLoop()\n",
        "data = CoraGraphDataset(transform=transform)\n",
        "graph = data[0].to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlHds92MImw2",
        "outputId": "0fdb14db-40c5-4adb-a945-b73041e9374f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([False])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph.has_edges_between([0], [1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pagerank(net, weights={}, q=0.5, eps=0.01, maxIters=500, verbose=False, weightName='weight'):\n",
        "\n",
        "    incomingTeleProb = {}\n",
        "    prevVisitProb = {}\n",
        "    currVisitProb = {}\n",
        "    N = net.number_of_nodes()\n",
        "\n",
        "    totWeight = sum([w for v, w in weights.items()])\n",
        "\n",
        "    if totWeight == 0:\n",
        "        incomingTeleProb = dict.fromkeys(net, 1.0 / N)\n",
        "        prevVisitProb = incomingTeleProb.copy()\n",
        "        currVisitProb = incomingTeleProb.copy()\n",
        "    else:\n",
        "        minPosWeight = 1.0\n",
        "        for v, weight in weights.items():\n",
        "            if weight == 0:\n",
        "                continue\n",
        "            minPosWeight = min(minPosWeight, 1.0 * weight / totWeight)\n",
        "\n",
        "        smallWeight = minPosWeight / (10 ** 6)\n",
        "\n",
        "        for v in net.nodes():\n",
        "            weight = weights.get(v, 0.0)\n",
        "            incomingTeleProb[v] = 1.0 * \\\n",
        "                (weight + smallWeight) / (totWeight + smallWeight * N)\n",
        "        prevVisitProb = incomingTeleProb.copy()\n",
        "        currVisitProb = incomingTeleProb.copy()\n",
        "\n",
        "    outDeg = {}\n",
        "    zeroDegNodes = set()\n",
        "    for v in net.nodes():\n",
        "        outDeg[v] = 1.0 * net.out_degree(v, weight=weightName)\n",
        "        if outDeg[v] == 0:\n",
        "            zeroDegNodes.add(v)\n",
        "\n",
        "    iters = 0\n",
        "    finished = False\n",
        "    while not finished:\n",
        "        iters += 1\n",
        "        prevVisitProb = currVisitProb.copy()\n",
        "        maxDiff = 0\n",
        "\n",
        "        zSum = sum([prevVisitProb[x] for x in zeroDegNodes]) / N\n",
        "\n",
        "        for v in net.nodes():\n",
        "            eSum = 0\n",
        "            for u in net.predecessors(v):\n",
        "                # Handle missing 'weight' attribute by assuming it's 1.0\n",
        "                w_uv = 1.0 * net[u][v].get(weightName, 1.0)\n",
        "                eSum += w_uv / outDeg[u] * prevVisitProb[u]\n",
        "\n",
        "            currVisitProb[v] = q * incomingTeleProb[v] + \\\n",
        "                (1 - q) * (eSum + zSum)\n",
        "            maxDiff = max(maxDiff, abs(\n",
        "                (prevVisitProb[v] - currVisitProb[v]) / currVisitProb[v]))\n",
        "\n",
        "        if verbose:\n",
        "            print('\\tIteration %d, max difference %f' % (iters, maxDiff))\n",
        "            if maxDiff < eps:\n",
        "                print('PageRank converged after %d iterations, max difference %f.' % (\n",
        "                    iters, maxDiff))\n",
        "\n",
        "        if iters >= maxIters:\n",
        "            print(\n",
        "                'WARNING: PageRank terminated because max iters (%d) was reached.' % (maxIters))\n",
        "\n",
        "        finished = (maxDiff < eps) or (iters >= maxIters)\n",
        "\n",
        "    return currVisitProb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{0: 0.00036052501717742354, 1: 0.0004028661685851042, 2: 0.00042950848679268656, 3: 0.00036927621861152144, 4: 0.00035722113227557104, 5: 0.0003712386186454783, 6: 0.0003268850780867359, 7: 0.00036927621861152144, 8: 0.0003311581532613355, 9: 0.00033604025850795356, 10: 0.00036371323256205717, 11: 0.000288389696149014, 12: 0.00043213180891019446, 13: 0.0002743402678377291, 14: 0.00034921938544734196, 15: 0.00032836603464356077, 16: 0.0003538052070250879, 17: 0.0004068781812034786, 18: 0.00040390072061154304, 19: 0.00032241995629062254, 20: 0.0004061654847465194, 21: 0.0003539112306090031, 22: 0.00034467036618850356, 23: 0.0003096395294460056, 24: 0.00041858940265620875, 25: 0.00036733492195301423, 26: 0.0003999421957619074, 27: 0.0003681953744845908, 28: 0.00029932130794493427, 29: 0.0002887645750047761, 30: 0.00041450899066961065, 31: 0.00036927621861152144, 32: 0.00033383405751729, 33: 0.0005187340768002362, 34: 0.00026996422662422273, 35: 0.0003224533895005945, 36: 0.0004768293808207571, 37: 0.00037606959703292487, 38: 0.00035217641800171847, 39: 0.0004945397490078599, 40: 0.00038730284297815523, 41: 0.00032472126123083117, 42: 0.00034485370986354663, 43: 0.0004060946144855556, 44: 0.00043914905482728454, 45: 0.00039784751807620726, 46: 0.0003187743148721367, 47: 0.00036247207742709, 48: 0.0005413651042123272, 49: 0.0002840042691932767, 50: 0.00028573652589228147, 51: 0.0004540135896849867, 52: 0.00039836662470385173, 53: 0.00028586052597703436, 54: 0.0003027083710199382, 55: 0.0006094126769193187, 56: 0.0003491417669325926, 57: 0.00028845624748925904, 58: 0.00030391473949927097, 59: 0.0005168430936560715, 60: 0.00032327214763806013, 61: 0.00036850492823086204, 62: 0.0003017454097035322, 63: 0.0003310539499644595, 64: 0.00035062763612868846, 65: 0.0005548265835855545, 66: 0.00036927621861152144, 67: 0.00040711582964055487, 68: 0.0002951689726588089, 69: 0.00039123031992262334, 70: 0.00031840968028595487, 71: 0.000366320479534821, 72: 0.00027101352587860034, 73: 0.00056359759878537, 74: 0.0004717550635925776, 75: 0.0003985689285201998, 76: 0.0002946804809730116, 77: 0.00032357869505071456, 78: 0.0003976660554580539, 79: 0.00031896922865863445, 80: 0.00035266525595826505, 81: 0.0003685350576537839, 82: 0.0003139194962911303, 83: 0.0003307693554258013, 84: 0.0003681102301861935, 85: 0.0004290425652388559, 86: 0.0003175236618261683, 87: 0.00036724878346039967, 88: 0.0014253098386745994, 89: 0.0005658245306669845, 90: 0.0003191472884126221, 91: 0.0003985414044142991, 92: 0.0003389767382296708, 93: 0.0004252131013354159, 94: 0.0006772378023568576, 95: 0.0008044226048225049, 96: 0.0002942892379304144, 97: 0.00031547294054302396, 98: 0.0003224597978308461, 99: 0.00044728143415037494, 100: 0.00028351917607499224, 101: 0.0003114618944429926, 102: 0.0004697746927546711, 103: 0.0003704594198611445, 104: 0.0003818519514973349, 105: 0.0003338152794694278, 106: 0.00036927621861152144, 107: 0.00039910020202709264, 108: 0.00035376459980213614, 109: 0.001108850997674504, 110: 0.00042544770663813556, 111: 0.0004210877991775964, 112: 0.00029083474037009543, 113: 0.0003325896725664968, 114: 0.0003869712689959701, 115: 0.00027736149847663676, 116: 0.0003541801423482515, 117: 0.00036927621861152144, 118: 0.0008817059124743143, 119: 0.0004697504297677721, 120: 0.00035016546920249017, 121: 0.00048498166225610905, 122: 0.00036292777195072673, 123: 0.00037014207083932665, 124: 0.00038397866053477216, 125: 0.0003336002990556866, 126: 0.00036529475932588174, 127: 0.0003094225441127176, 128: 0.00039060464243340673, 129: 0.0003008561712906766, 130: 0.00032798004044190207, 131: 0.0003136471896637477, 132: 0.00038845187996111206, 133: 0.00045137363409113895, 134: 0.00030288533464462963, 135: 0.0003263434132686192, 136: 0.0002915738278160958, 137: 0.0003671538624421088, 138: 0.0002770323929408996, 139: 0.00041938287566360217, 140: 0.00031286806198624345, 141: 0.000315773629273732, 142: 0.0003526518586423562, 143: 0.00028906559451598687, 144: 0.00036115712267816556, 145: 0.0004720976990280645, 146: 0.0003713623113977621, 147: 0.0003151771553219561, 148: 0.0004391628248691914, 149: 0.00031501527873706065, 150: 0.00034179731151831556, 151: 0.00032440809865200333, 152: 0.00034723962135014554, 153: 0.0003255254358364918, 154: 0.0006217796100314123, 155: 0.0004280306032410091, 156: 0.00039604810645069036, 157: 0.00033458766105818303, 158: 0.00031429758809290755, 159: 0.00030442663104091023, 160: 0.0003980517610908276, 161: 0.0003069050332973931, 162: 0.00037220109188094676, 163: 0.00032697974589023486, 164: 0.0002932389511327194, 165: 0.0003474004784168006, 166: 0.0003615130707847918, 167: 0.000437315139166226, 168: 0.000378970888059917, 169: 0.0003299913272132311, 170: 0.00027678388457535476, 171: 0.0003532131658755247, 172: 0.00035281469506068984, 173: 0.00033072279140092053, 174: 0.00034943726227161043, 175: 0.00044128549383377295, 176: 0.00028773539455740266, 177: 0.0003954837309207576, 178: 0.00029698016135000797, 179: 0.0003137098831450543, 180: 0.000566439141297328, 181: 0.00032931839242126583, 182: 0.00033390172872976344, 183: 0.0003742451270541243, 184: 0.00036927621861152144, 185: 0.0002953641466529485, 186: 0.0005159093005817378, 187: 0.00036927621861152144, 188: 0.0002938690050926506, 189: 0.00035878104514630495, 190: 0.0003723401656008166, 191: 0.00039331182658947875, 192: 0.0002929464866578386, 193: 0.00029963831047634913, 194: 0.0004587846611710134, 195: 0.0003205103556466047, 196: 0.0002989484005894347, 197: 0.00033819235991995643, 198: 0.0003239460474867131, 199: 0.0003153337851882702, 200: 0.00036927621861152144, 201: 0.00043862041886483266, 202: 0.0003939659954857014, 203: 0.0003484397327177777, 204: 0.0002747720933213988, 205: 0.00027944874594159193, 206: 0.00034579336115256624, 207: 0.00029698016135000797, 208: 0.00036927621861152144, 209: 0.000313641088273758, 210: 0.0005931680976653514, 211: 0.0003933916682099303, 212: 0.0003220871174529173, 213: 0.00036115712267816556, 214: 0.00030075304537363856, 215: 0.00027972142854919496, 216: 0.000272075364032007, 217: 0.00038115404826356684, 218: 0.0006041927310202716, 219: 0.0002808774522081916, 220: 0.00038484248305860233, 221: 0.0002859821436747897, 222: 0.00036927621861152144, 223: 0.000304261052623319, 224: 0.0002720455888588593, 225: 0.00036927621861152144, 226: 0.00027650981013928024, 227: 0.0003141753106676516, 228: 0.0004087201418299729, 229: 0.0003736086495806851, 230: 0.0003880077277228874, 231: 0.0005412249905706288, 232: 0.000346139071703408, 233: 0.0002983928549678403, 234: 0.00032521031803542247, 235: 0.00027650981013928024, 236: 0.0004807580794197107, 237: 0.00031694045050226644, 238: 0.0002997444060725639, 239: 0.0007313458945844527, 240: 0.00034981707888474123, 241: 0.00028573652589228147, 242: 0.0003139346295540048, 243: 0.00046482210838561064, 244: 0.0003059813239931189, 245: 0.0003907454031786959, 246: 0.0003413258629976327, 247: 0.00036927621861152144, 248: 0.00034088836628546845, 249: 0.00032389505198893176, 250: 0.00036927621861152144, 251: 0.0003849227323086344, 252: 0.0003232082249826912, 253: 0.00028617542023058563, 254: 0.0003090942290204584, 255: 0.000339931148963563, 256: 0.0002785353654952849, 257: 0.0003473059235623785, 258: 0.0004332931087637329, 259: 0.00036927621861152144, 260: 0.0003332246881613214, 261: 0.0003252708170939795, 262: 0.0002839257340592054, 263: 0.0003028397146852055, 264: 0.0003350564941732176, 265: 0.00028573652589228147, 266: 0.00037474990356197607, 267: 0.00041635129015172754, 268: 0.00027905413982828235, 269: 0.0005062721060433614, 270: 0.0003639740936405329, 271: 0.00041699203933620875, 272: 0.00032234511256495554, 273: 0.00031542216424605837, 274: 0.0003033602724854215, 275: 0.0003279091393685295, 276: 0.0003241629949014014, 277: 0.00034745517082821197, 278: 0.00029251214859648444, 279: 0.0006222622746999532, 280: 0.00028235835345332517, 281: 0.0005344794284260512, 282: 0.0003285163629162779, 283: 0.0003450431330026833, 284: 0.0004356663399314625, 285: 0.00032012533113980133, 286: 0.00030816929901407756, 287: 0.00036927621861152144, 288: 0.0003299489870165913, 289: 0.0002960296348646255, 290: 0.000352689135506285, 291: 0.0002781447469016584, 292: 0.000340870241484612, 293: 0.00032278588904076193, 294: 0.000301450009652401, 295: 0.0002987197105800395, 296: 0.00030889263831767325, 297: 0.0003689804233912235, 298: 0.00033315273963569576, 299: 0.0003929771366988763, 300: 0.0003157172585420369, 301: 0.00028204232267525883, 302: 0.00035253268735610033, 303: 0.0003146664419923611, 304: 0.00037420477261561054, 305: 0.00028017664659017467, 306: 0.00255466028922022, 307: 0.00036927621861152144, 308: 0.00034513896027817836, 309: 0.0003184615083587207, 310: 0.0008191819894079677, 311: 0.0004795972690434518, 312: 0.00031567977283952466, 313: 0.0003121184499308458, 314: 0.0002871922827706952, 315: 0.00037804197372402477, 316: 0.0005566723295977654, 317: 0.00028124528859546654, 318: 0.0002980564444096603, 319: 0.0003044495090622736, 320: 0.0003462091535144272, 321: 0.000359402519513931, 322: 0.0002917008624687475, 323: 0.0003889771313815585, 324: 0.0003398503251520696, 325: 0.0003041143344421006, 326: 0.000296797573983627, 327: 0.00041060741718896855, 328: 0.0003398221662274417, 329: 0.0003437933243950615, 330: 0.00030130915676758284, 331: 0.00030840809029029686, 332: 0.0004000561868011557, 333: 0.0003397437482472789, 334: 0.00029193767275057463, 335: 0.0002734473253889376, 336: 0.0002837161823885316, 337: 0.00031436368794968036, 338: 0.0003123489521902048, 339: 0.00032509074511693787, 340: 0.00030402002284351593, 341: 0.0003620519017345717, 342: 0.0002886229136167725, 343: 0.00030641644254026735, 344: 0.00036851627383920597, 345: 0.00029698016135000797, 346: 0.0003037533902639529, 347: 0.0003337445462583232, 348: 0.0004290505671071892, 349: 0.0004022413204658596, 350: 0.0006191474480390763, 351: 0.0002745999834573198, 352: 0.0003266667507356697, 353: 0.0004006829283146007, 354: 0.00036839870401936476, 355: 0.0003247877916408631, 356: 0.00042139264130131363, 357: 0.0003208145337237083, 358: 0.0002976396079405279, 359: 0.0002883408974816291, 360: 0.0002933099801051616, 361: 0.000313641088273758, 362: 0.00028082282868068333, 363: 0.0002994469114081603, 364: 0.0005700915071830089, 365: 0.000391133452034274, 366: 0.0003046091646218697, 367: 0.00028181731732860834, 368: 0.00047542736033679767, 369: 0.000340870241484612, 370: 0.0003789068366752933, 371: 0.00045913350063904885, 372: 0.0003974262351881576, 373: 0.0003305038203814633, 374: 0.0005604219776298399, 375: 0.00036135024547937205, 376: 0.00035600379623906006, 377: 0.00035667052838702805, 378: 0.00033202857586979136, 379: 0.0004067186858028402, 380: 0.00036927621861152144, 381: 0.0003194613955737617, 382: 0.000299173401734162, 383: 0.00034153440983800945, 384: 0.0002676526591315556, 385: 0.0003976821957384309, 386: 0.0003337842301536816, 387: 0.00035285403530386865, 388: 0.0003339453252231768, 389: 0.00041659486762035723, 390: 0.00036927621861152144, 391: 0.00035300595633384137, 392: 0.0004226366610996554, 393: 0.0002867624080543804, 394: 0.0003914887303934091, 395: 0.0003087488931821041, 396: 0.00035119607765125794, 397: 0.00037174982226051043, 398: 0.0003651855722788921, 399: 0.00036057319690251826, 400: 0.00029779455649265793, 401: 0.00042011365112631374, 402: 0.0004016321852334644, 403: 0.0004994212349141026, 404: 0.00036927621861152144, 405: 0.0003601872623082606, 406: 0.0004236567156796892, 407: 0.00032512168562555356, 408: 0.0003727187421659825, 409: 0.00033335685222951703, 410: 0.000341836882246251, 411: 0.0003833966812725251, 412: 0.00031063703660346196, 413: 0.0002929676038338684, 414: 0.0002864674959117747, 415: 0.0009428695569937941, 416: 0.00027353917786566897, 417: 0.0003526953898730599, 418: 0.0003025572748019058, 419: 0.0003133532372003146, 420: 0.0003153337851882702, 421: 0.0003100800226954688, 422: 0.00036927621861152144, 423: 0.0003894646075274232, 424: 0.00028317348128584143, 425: 0.00029583581541683495, 426: 0.0003836951636996249, 427: 0.00029457469247993594, 428: 0.0003398357723980478, 429: 0.0008232104372911348, 430: 0.0002863327266214203, 431: 0.00036927621861152144, 432: 0.0003740904642021205, 433: 0.0002782830095729163, 434: 0.00041967478208432225, 435: 0.0003055523406090754, 436: 0.0008759928989114239, 437: 0.00041062032473422806, 438: 0.000442403063470133, 439: 0.0003136397501873861, 440: 0.0004506280053909207, 441: 0.0006618497346871544, 442: 0.0005162031223031944, 443: 0.0003786451129650188, 444: 0.0005008517670817647, 445: 0.0003861437122577651, 446: 0.0003221115937305668, 447: 0.00038482574711964137, 448: 0.0003212160602805629, 449: 0.00039096827815876696, 450: 0.0003103328593402063, 451: 0.0003288243671433173, 452: 0.00031090713117769666, 453: 0.0003365694086939349, 454: 0.0005980951158637872, 455: 0.00035344556510092545, 456: 0.0004824360695364536, 457: 0.00036753684120214636, 458: 0.0005244530139035049, 459: 0.0002986451102214957, 460: 0.00038100812459578595, 461: 0.0002922822356191562, 462: 0.00036927621861152144, 463: 0.0003203535282565714, 464: 0.00033659490597475275, 465: 0.00044788003894764914, 466: 0.00037988737964674273, 467: 0.0003336566198285445, 468: 0.0003267345686980901, 469: 0.0003593840131028707, 470: 0.0003891779142178427, 471: 0.0003171788290923201, 472: 0.0004946845756155729, 473: 0.0004297188255740688, 474: 0.00036927621861152144, 475: 0.00031072480625169726, 476: 0.0004373388048088638, 477: 0.00036927621861152144, 478: 0.0003185448820833258, 479: 0.0003062612590110992, 480: 0.00038087710593824684, 481: 0.0003321648483978927, 482: 0.0005130318888749867, 483: 0.0003189944390019357, 484: 0.00043187898335757746, 485: 0.0004154065142990056, 486: 0.00034260821249057586, 487: 0.0005183143970626484, 488: 0.0003192933434441173, 489: 0.00030439859383475665, 490: 0.000498900691475393, 491: 0.00032916404305543385, 492: 0.0004784896715758259, 493: 0.0003830151457975548, 494: 0.00029868074252646117, 495: 0.00035711514123587583, 496: 0.0003383560290977905, 497: 0.0002959005548738833, 498: 0.0003875126489891678, 499: 0.0003428137192505971, 500: 0.0002938324103306423, 501: 0.0002697748371050442, 502: 0.00034898174974165174, 503: 0.0003006511094886011, 504: 0.00031206995232353665, 505: 0.0005850738542239097, 506: 0.0003871313063007406, 507: 0.000811103239638392, 508: 0.0003142989926519441, 509: 0.00032509153116942876, 510: 0.00028569775896169277, 511: 0.0002918255938508414, 512: 0.00034981707888474123, 513: 0.0003066124490506427, 514: 0.00039302720310839675, 515: 0.0004189676331180853, 516: 0.0003569228391356328, 517: 0.0003541907805824839, 518: 0.0003673486350640512, 519: 0.000536573657338255, 520: 0.00036927621861152144, 521: 0.0002733527023749913, 522: 0.0003167302236301681, 523: 0.00039367207962450693, 524: 0.00034178331997315693, 525: 0.0006347130268366669, 526: 0.00033086768069684995, 527: 0.0003047986469589845, 528: 0.00032926645245792176, 529: 0.00034348070885131007, 530: 0.0002917391784718318, 531: 0.00031561768906842745, 532: 0.00031111721337158946, 533: 0.00029201979832982285, 534: 0.00038555676882347386, 535: 0.0003369896678650034, 536: 0.0002854624642556881, 537: 0.0004145339185852791, 538: 0.00036927621861152144, 539: 0.00029956325204055805, 540: 0.00045734012167193835, 541: 0.00040766903218012227, 542: 0.0003064979551482337, 543: 0.0003091300293528756, 544: 0.00043300321947281563, 545: 0.00036927621861152144, 546: 0.00031260412088199627, 547: 0.0002732210389688669, 548: 0.00046473908513653105, 549: 0.00037272387989728106, 550: 0.00030303348557014824, 551: 0.00028899567839421597, 552: 0.00031961442038383415, 553: 0.0004261669116403374, 554: 0.00034142203752728496, 555: 0.0002875799541599204, 556: 0.00027617321524451915, 557: 0.0002741010555442501, 558: 0.0004091425479844079, 559: 0.00032321954603778023, 560: 0.0004028466490272687, 561: 0.00029219919656059407, 562: 0.00029696712986063497, 563: 0.000327860202536706, 564: 0.00032024447858549905, 565: 0.0006473788485586006, 566: 0.00028418335185328297, 567: 0.0004804985238567721, 568: 0.0003672388721375656, 569: 0.000365032355061999, 570: 0.00031816363101555263, 571: 0.00031726064310051764, 572: 0.0003013374413001276, 573: 0.0003671811775094479, 574: 0.0003006511094886011, 575: 0.00030567759977496837, 576: 0.00038182530973335845, 577: 0.0006854356126691785, 578: 0.0004117733225109459, 579: 0.0003511610856147881, 580: 0.00041161906130484396, 581: 0.00036578036551462054, 582: 0.00046560786965444583, 583: 0.00032921582705249496, 584: 0.00038978667261508617, 585: 0.00033570578819577416, 586: 0.000341797886790916, 587: 0.00036927621861152144, 588: 0.00032696936116699516, 589: 0.00032500796000471677, 590: 0.0002884323582480951, 591: 0.0004789632758852801, 592: 0.00036927621861152144, 593: 0.00030831662760701323, 594: 0.0003398921632877017, 595: 0.00031025251317690983, 596: 0.0004184257167315729, 597: 0.00030645678109049424, 598: 0.001387786749884933, 599: 0.0002941384602129438, 600: 0.0002790320114582087, 601: 0.0003097018604773572, 602: 0.00033202857586979136, 603: 0.0004535539427680133, 604: 0.0003432269085292624, 605: 0.0003059121630290676, 606: 0.0004899856351636567, 607: 0.0003673899160539254, 608: 0.0003232980702851314, 609: 0.0004572576194076021, 610: 0.0005142694726787742, 611: 0.00036927621861152144, 612: 0.0003108065805918127, 613: 0.0002791071740230047, 614: 0.0003753328964043037, 615: 0.00031179865520974574, 616: 0.00031462021345112683, 617: 0.0003090805751515924, 618: 0.00037610792113663223, 619: 0.0002831468576787974, 620: 0.0003953680819598004, 621: 0.0003624303491442433, 622: 0.0003722548407032002, 623: 0.0004127657978830259, 624: 0.0003150627376342725, 625: 0.00036927621861152144, 626: 0.0003618956316809041, 627: 0.00031937432480272387, 628: 0.0003467861923283885, 629: 0.0003391437983231249, 630: 0.00027736149847663676, 631: 0.00029724458128454573, 632: 0.00032716426743943715, 633: 0.00034579083795379485, 634: 0.0003059438694031689, 635: 0.0003178715434136053, 636: 0.00030566637437058803, 637: 0.00029980641781616676, 638: 0.0004341664201237839, 639: 0.000334287997364158, 640: 0.0004651961697591931, 641: 0.00036927621861152144, 642: 0.00031153548060433713, 643: 0.0003668044015797625, 644: 0.0004123926949767397, 645: 0.0006430600992360976, 646: 0.0002909979817226195, 647: 0.00037793323070655255, 648: 0.00028573652589228147, 649: 0.0003226609501083716, 650: 0.00043407122833026183, 651: 0.0004364573022545524, 652: 0.00032098547992349366, 653: 0.00036927621861152144, 654: 0.0003135129440695277, 655: 0.0002785257250263883, 656: 0.0002910877104763789, 657: 0.0005941955237483567, 658: 0.0002930241063567911, 659: 0.00036965424138670974, 660: 0.0002711465398080951, 661: 0.0008715120292800073, 662: 0.00036927621861152144, 663: 0.0003398921632877017, 664: 0.00027650981013928024, 665: 0.00037657418903778804, 666: 0.00033383405751729, 667: 0.000325940602637004, 668: 0.0002769770715903288, 669: 0.0003451203972912173, 670: 0.00038366217435894335, 671: 0.00033509569241131577, 672: 0.00034020770326109495, 673: 0.00028418335185328297, 674: 0.00028844784377689574, 675: 0.00029988775743521306, 676: 0.00036610738639204315, 677: 0.000340870241484612, 678: 0.00028994428084327744, 679: 0.0002924019558195915, 680: 0.0002786025288345706, 681: 0.0003820059244103314, 682: 0.00027287190114142726, 683: 0.0003136471896637477, 684: 0.0002731525867368404, 685: 0.00030327557904144775, 686: 0.0002975040864342509, 687: 0.0006328305006379765, 688: 0.0002833597518037874, 689: 0.00033364000785925845, 690: 0.00031026515008036697, 691: 0.00028258642999644213, 692: 0.00036927621861152144, 693: 0.00039586539773183674, 694: 0.0003285994803218478, 695: 0.000480699393790497, 696: 0.0003654085992608726, 697: 0.00029121491373809477, 698: 0.000365458964563394, 699: 0.00030214117232849806, 700: 0.00036927621861152144, 701: 0.0003848817784340223, 702: 0.0006299918880382951, 703: 0.00036149392707403424, 704: 0.0003960325268741343, 705: 0.0003124883001684374, 706: 0.0003604591835874796, 707: 0.00030898386197473934, 708: 0.00032579222325543794, 709: 0.0003049145535792859, 710: 0.0003763020331337979, 711: 0.00060263845335081, 712: 0.00034120409367513, 713: 0.00036927621861152144, 714: 0.0003288237027778734, 715: 0.00028382159988878515, 716: 0.00035942748995484655, 717: 0.00028844784377689574, 718: 0.00038570567274331523, 719: 0.0003368470669976932, 720: 0.00031208836358451067, 721: 0.00036927621861152144, 722: 0.00038343495454134233, 723: 0.00033604025850795356, 724: 0.00031512038859421736, 725: 0.00043590167705955066, 726: 0.00032432116078854136, 727: 0.00033719041815528895, 728: 0.00035748135648323685, 729: 0.0004015457771211823, 730: 0.00032757630754164525, 731: 0.0003421233487430099, 732: 0.0003984818967473843, 733: 0.0012815829726092937, 734: 0.0005529795586616017, 735: 0.0006091423393787613, 736: 0.0005102563279121668, 737: 0.0004178364868468299, 738: 0.00036623220577157326, 739: 0.00048660381791991626, 740: 0.0003299973198231638, 741: 0.0004194539641565279, 742: 0.000285883399822847, 743: 0.0003906072722710562, 744: 0.00031866803153365327, 745: 0.00042375152831137944, 746: 0.0003343140948993046, 747: 0.00038194753180016586, 748: 0.0006241108963785126, 749: 0.00033696869166472624, 750: 0.00028573652589228147, 751: 0.00035001724357735446, 752: 0.00029959838147611436, 753: 0.00030536641148029857, 754: 0.00027505091634372723, 755: 0.0003716999568996171, 756: 0.0005815389142025873, 757: 0.0003485997420682987, 758: 0.00032965562179120776, 759: 0.0002730042207617968, 760: 0.00046479188285791973, 761: 0.0002958822422421881, 762: 0.0003018425466869245, 763: 0.00037551866265042334, 764: 0.0003871622408837651, 765: 0.0003087488931821041, 766: 0.00036760900090603786, 767: 0.00037587468735175754, 768: 0.00031614396928217694, 769: 0.0002814748626320905, 770: 0.0002835722616118762, 771: 0.0003267466730594321, 772: 0.0003224597978308461, 773: 0.00026977927936662217, 774: 0.00033570578819577416, 775: 0.00031988517813590956, 776: 0.0002851682272741791, 777: 0.00028827108450593257, 778: 0.00032313307968111357, 779: 0.00040567252799532144, 780: 0.00036927621861152144, 781: 0.0003219714347367259, 782: 0.0003331892217053999, 783: 0.00036722764463403303, 784: 0.0003063721854514115, 785: 0.00030337186202252783, 786: 0.00036927621861152144, 787: 0.00033180324203966834, 788: 0.0003886120932554254, 789: 0.0002823708580093844, 790: 0.000385750641177858, 791: 0.00029846994966105497, 792: 0.0003679635324111458, 793: 0.0003662766352321897, 794: 0.0003704190254920582, 795: 0.0003080515418723413, 796: 0.00031975556695873215, 797: 0.0003547185026173465, 798: 0.0004296562494158441, 799: 0.0003016299269378805, 800: 0.000378378195851633, 801: 0.000390057481933976, 802: 0.0003105182665397086, 803: 0.0003730685397565608, 804: 0.00030466495285528866, 805: 0.0003631617911802285, 806: 0.00032391591313930736, 807: 0.00034734392896865245, 808: 0.0003381340527809564, 809: 0.00039671618611748316, 810: 0.00031009586834674757, 811: 0.0003701125277790763, 812: 0.0003188471347043581, 813: 0.00035397053065560106, 814: 0.00033338071727142876, 815: 0.00037945952134165665, 816: 0.0003962705518466179, 817: 0.0005363435964632823, 818: 0.00029340012894150686, 819: 0.000345349332046803, 820: 0.0003156881578100174, 821: 0.00036927621861152144, 822: 0.00028663520773346973, 823: 0.0003337992438269858, 824: 0.00032837765741904144, 825: 0.0002760755805814627, 826: 0.0003735274638504494, 827: 0.00038241725673385165, 828: 0.00030686904659166627, 829: 0.0003229112581667339, 830: 0.00031405867126751475, 831: 0.0005430241957080798, 832: 0.00036927621861152144, 833: 0.0007605802686102703, 834: 0.00038519649432476263, 835: 0.0003453310674024384, 836: 0.0004428282353207894, 837: 0.00037872983965898574, 838: 0.0003736736163924312, 839: 0.0003450534079096043, 840: 0.000322988047678672, 841: 0.0002695627194846106, 842: 0.00037043378750939654, 843: 0.0004017184788934042, 844: 0.0003094763063638662, 845: 0.00029809089886357357, 846: 0.00034534520694969907, 847: 0.00032097870934196643, 848: 0.00033558783223858093, 849: 0.0003579026275239806, 850: 0.00034891464301329124, 851: 0.00030659371842072143, 852: 0.0003064520938923124, 853: 0.000292292773857607, 854: 0.0004278818342641691, 855: 0.00034622925590870523, 856: 0.00032082186190989035, 857: 0.0004100474441741161, 858: 0.00034516389039556474, 859: 0.0004245631240180144, 860: 0.0003111875343690818, 861: 0.0004034974577437682, 862: 0.000289401822265076, 863: 0.0003843006134804246, 864: 0.00035184052201712133, 865: 0.00032613978408847045, 866: 0.00032990263212361335, 867: 0.0005223861729044053, 868: 0.000529883120017919, 869: 0.0003664825723991331, 870: 0.0002965838152888499, 871: 0.0003547100778438486, 872: 0.0002957680825851719, 873: 0.00041584204634437195, 874: 0.00027650981013928024, 875: 0.00038158448233904454, 876: 0.00042586068973836887, 877: 0.00036596298166513655, 878: 0.000308674587405228, 879: 0.000304326035082081, 880: 0.00027307857153334615, 881: 0.00032095715946028577, 882: 0.00026996422662422273, 883: 0.00038555676882347386, 884: 0.00029410263417962426, 885: 0.00043530192482513474, 886: 0.00033855152960801237, 887: 0.00031654174047429586, 888: 0.0003993774446558605, 889: 0.00048693927840336783, 890: 0.00031111033926080073, 891: 0.00033953857082718697, 892: 0.00027608498346949996, 893: 0.0003839317170375293, 894: 0.00033291226242082746, 895: 0.00041492396707353625, 896: 0.00029601039789774707, 897: 0.00047025599456977267, 898: 0.00041313880113849486, 899: 0.0003721377877560998, 900: 0.0003959931780163011, 901: 0.0003941564238941947, 902: 0.0003020453678637725, 903: 0.00036439553952180955, 904: 0.00043513736321718007, 905: 0.000308318466703276, 906: 0.0003221806785862236, 907: 0.0003019189309694786, 908: 0.0003420526085390405, 909: 0.0003321752585476152, 910: 0.0003485941434740209, 911: 0.0003568160281540687, 912: 0.0002863327266214203, 913: 0.00031052537074234847, 914: 0.0003110961396318448, 915: 0.0003366297372534166, 916: 0.0003040726817950982, 917: 0.00036927621861152144, 918: 0.00032622082971543135, 919: 0.00026996422662422273, 920: 0.0003154029669611378, 921: 0.0003379544963159953, 922: 0.00033960313611661446, 923: 0.00031222968126037604, 924: 0.000308760984037619, 925: 0.00032241394805816, 926: 0.00030173100915377763, 927: 0.00033115654297978585, 928: 0.000349510029102872, 929: 0.0003397437482472789, 930: 0.00036927621861152144, 931: 0.00030593431659738096, 932: 0.00036927621861152144, 933: 0.0003370561826216823, 934: 0.00029988775743521306, 935: 0.0003607770471694906, 936: 0.00039550683540229005, 937: 0.00034165529524806103, 938: 0.0003693398043131429, 939: 0.000340870241484612, 940: 0.0002905649003181634, 941: 0.00033071140664403867, 942: 0.00030274916466226326, 943: 0.0002969079598722948, 944: 0.00030737841135360236, 945: 0.0003388376695567403, 946: 0.0003501905647376887, 947: 0.00036927621861152144, 948: 0.00032653961745908965, 949: 0.0003119929318497287, 950: 0.0003269901887360734, 951: 0.000293914983077969, 952: 0.0003397847681721427, 953: 0.0003976821957384309, 954: 0.00042608817286534035, 955: 0.000301508259148354, 956: 0.0003198442272808104, 957: 0.0004915221800225935, 958: 0.00027709415665009115, 959: 0.00036927621861152144, 960: 0.00038304426639351925, 961: 0.000415042537549977, 962: 0.00045349808552231947, 963: 0.0008288651686882188, 964: 0.0003816627013295367, 965: 0.0003548924127708607, 966: 0.0002747720933213988, 967: 0.00036927621861152144, 968: 0.0003446674279569525, 969: 0.0003144454221863176, 970: 0.00035010480014556673, 971: 0.0003171084205262076, 972: 0.0003693326395573052, 973: 0.0008407712968134724, 974: 0.00036927621861152144, 975: 0.0003167302236301681, 976: 0.0003253056900928219, 977: 0.00029928947080353407, 978: 0.00028150465966418484, 979: 0.00041736028230199697, 980: 0.0003204267808565611, 981: 0.0003168757489863166, 982: 0.00041312547596140036, 983: 0.0002863327266214203, 984: 0.00038197432255738096, 985: 0.00037922490216724255, 986: 0.00036927621861152144, 987: 0.000323704088497511, 988: 0.00028551224564018986, 989: 0.0003625188371349952, 990: 0.0002935283034389513, 991: 0.00036927621861152144, 992: 0.0003082239513040005, 993: 0.00028527393779299057, 994: 0.00038242056906624374, 995: 0.0002999578402442302, 996: 0.00036652198267747007, 997: 0.0003742451270541243, 998: 0.00044970155907793, 999: 0.0003037533902639529, 1000: 0.00035876299947616735, 1001: 0.00032999152293577635, 1002: 0.00041506681747110124, 1003: 0.00047783605698190486, 1004: 0.00034418420370649854, 1005: 0.00036927621861152144, 1006: 0.0003489615977349869, 1007: 0.0003332575381345054, 1008: 0.0004019879985102456, 1009: 0.0002773160337393508, 1010: 0.00029815481210957725, 1011: 0.00030433912047613443, 1012: 0.0003442749621735755, 1013: 0.0013748792997242253, 1014: 0.00028863439046433075, 1015: 0.0006890734781810847, 1016: 0.00041312547596140036, 1017: 0.0003419915609377462, 1018: 0.0003233420625836341, 1019: 0.00032246379177745625, 1020: 0.00034522440453346146, 1021: 0.0002870886151833834, 1022: 0.0005343580152941513, 1023: 0.0003474812411468717, 1024: 0.00036927621861152144, 1025: 0.00029780876994287974, 1026: 0.00043280149781141673, 1027: 0.0002790320114582087, 1028: 0.0002803926881588954, 1029: 0.0003100722251388996, 1030: 0.0002935279178541139, 1031: 0.00030981715050362735, 1032: 0.00036927621861152144, 1033: 0.00030214534631087367, 1034: 0.00036927621861152144, 1035: 0.0004391714568865163, 1036: 0.000340870241484612, 1037: 0.00034747189665666176, 1038: 0.00033128058124127233, 1039: 0.00031214751040551617, 1040: 0.00029752639468760367, 1041: 0.00043745389060177215, 1042: 0.0010667334851868324, 1043: 0.0003292225321450118, 1044: 0.00036927621861152144, 1045: 0.0003721315912186098, 1046: 0.0004221836457968841, 1047: 0.00033323964590987483, 1048: 0.00036927621861152144, 1049: 0.0004171031865525571, 1050: 0.0006657219765412591, 1051: 0.0003715494631512989, 1052: 0.00027650981013928024, 1053: 0.0003112818737500018, 1054: 0.0004213217324521439, 1055: 0.000300314202605158, 1056: 0.00034258146396745506, 1057: 0.0003270863089202696, 1058: 0.0002971121265913935, 1059: 0.00036927621861152144, 1060: 0.00028573652589228147, 1061: 0.0003223460176831424, 1062: 0.00033098106034672425, 1063: 0.0003248984958011704, 1064: 0.00032948845702728826, 1065: 0.0005067677475188101, 1066: 0.0003038472195246591, 1067: 0.00045196387728146204, 1068: 0.00038244529627030395, 1069: 0.0002810747230047875, 1070: 0.0004087333058561786, 1071: 0.00031194873375747377, 1072: 0.0011019135669166212, 1073: 0.0003133890230754505, 1074: 0.00031591577571453715, 1075: 0.00030316022763335213, 1076: 0.0003251469868661348, 1077: 0.00030115722981594284, 1078: 0.00029950446615659886, 1079: 0.0003240232039910373, 1080: 0.0004553402528154713, 1081: 0.0003223774348955371, 1082: 0.00031299779214132137, 1083: 0.0004101966474638819, 1084: 0.0003160506925377532, 1085: 0.00032840867709601075, 1086: 0.00038573653459383754, 1087: 0.0002911868472946458, 1088: 0.0003075472582355482, 1089: 0.0003664197262757572, 1090: 0.0004192207870927992, 1091: 0.00031025251317690983, 1092: 0.00033185651105359685, 1093: 0.0004069611647950725, 1094: 0.00038369447756343006, 1095: 0.00043327532182848726, 1096: 0.0004313945508254389, 1097: 0.00036120855784722156, 1098: 0.0003783444900379722, 1099: 0.00032353363524157357, 1100: 0.0002732210389688669, 1101: 0.00040073980354325537, 1102: 0.00038179040785428205, 1103: 0.0008435750858807369, 1104: 0.0004006208754336225, 1105: 0.0003012146757496067, 1106: 0.000335106232019027, 1107: 0.00038504684505727085, 1108: 0.00036927621861152144, 1109: 0.00036225490658148534, 1110: 0.00031933605658771995, 1111: 0.00034878893435208073, 1112: 0.000340870241484612, 1113: 0.0003171084205262076, 1114: 0.0003276212979840855, 1115: 0.0004428025569005427, 1116: 0.00034596592659780093, 1117: 0.00037523316496878445, 1118: 0.0003048973947899395, 1119: 0.0006515169953920978, 1120: 0.0003533067131969566, 1121: 0.0003731456688770309, 1122: 0.0002962271899287936, 1123: 0.00027101352587860034, 1124: 0.00036220338554125497, 1125: 0.0002998207350826856, 1126: 0.0003039440241686631, 1127: 0.0003046091646218697, 1128: 0.0003204396224412548, 1129: 0.0003963503089622486, 1130: 0.0003167680512781347, 1131: 0.0008237141017902514, 1132: 0.00031026515008036697, 1133: 0.00032557396623108994, 1134: 0.0003570507577667487, 1135: 0.000401501215443723, 1136: 0.0003499633038540783, 1137: 0.00029473566136189994, 1138: 0.0003483888601027793, 1139: 0.00031301728068682466, 1140: 0.000294895716459514, 1141: 0.00030833664640612067, 1142: 0.0002918255042609467, 1143: 0.0003967421501968661, 1144: 0.00027932642155386965, 1145: 0.00028369605794913513, 1146: 0.00039787260022958756, 1147: 0.000449652695145606, 1148: 0.00034919123342905473, 1149: 0.00030864546103441835, 1150: 0.00030546170709811843, 1151: 0.00037366741364127184, 1152: 0.00035467019977928596, 1153: 0.00029504719775771146, 1154: 0.0005651189357965396, 1155: 0.00031338323092237907, 1156: 0.0003243959817395443, 1157: 0.00032537993411881235, 1158: 0.0005032835866755957, 1159: 0.00035333641955903453, 1160: 0.0003908885251315351, 1161: 0.0002935632563445581, 1162: 0.0003452084189997539, 1163: 0.0003046097220957785, 1164: 0.000295661856416213, 1165: 0.00032702797672116426, 1166: 0.00030136011989374963, 1167: 0.0002962276716324338, 1168: 0.0002934902047958025, 1169: 0.0009518046370692011, 1170: 0.00036927621861152144, 1171: 0.0005965263404413711, 1172: 0.0003166048534378727, 1173: 0.0003411083122916998, 1174: 0.0003608444906780653, 1175: 0.0003477524831338322, 1176: 0.00035989244016402627, 1177: 0.000342864636293541, 1178: 0.000342098450681524, 1179: 0.00032633147319481695, 1180: 0.0003356903007875453, 1181: 0.00036927621861152144, 1182: 0.00035249611543742334, 1183: 0.0004100072294806368, 1184: 0.00033709748783628435, 1185: 0.0005544029289417593, 1186: 0.0003907995235442657, 1187: 0.00036126106240015924, 1188: 0.00032668831925037516, 1189: 0.0003460336702310815, 1190: 0.00041942274893136833, 1191: 0.0003324889685072564, 1192: 0.0003386801896833807, 1193: 0.0002776601336004081, 1194: 0.0003415625831593091, 1195: 0.00038076384032276927, 1196: 0.00045480142281132275, 1197: 0.00029283358467975626, 1198: 0.0002758455422416263, 1199: 0.00042709140111539176, 1200: 0.0003637596512690297, 1201: 0.0003905019929445531, 1202: 0.00038303536568739236, 1203: 0.00045306322372493845, 1204: 0.00037817474164832067, 1205: 0.000348494226471474, 1206: 0.000276873709864238, 1207: 0.00031339281311283315, 1208: 0.00036927621861152144, 1209: 0.0003165646311107221, 1210: 0.00036927621861152144, 1211: 0.0003090942290204584, 1212: 0.00027650981013928024, 1213: 0.00041886511833295697, 1214: 0.0003845095643736704, 1215: 0.00031682042150174116, 1216: 0.00033414045044464585, 1217: 0.00033072361399015906, 1218: 0.0003589426595749241, 1219: 0.00034406470808517593, 1220: 0.0002810747230047875, 1221: 0.0004231456779053356, 1222: 0.00034183844799324495, 1223: 0.00034596592659780093, 1224: 0.0010512174189535564, 1225: 0.0003315426522879182, 1226: 0.00037359780574325107, 1227: 0.00028838062121656964, 1228: 0.00034133041310235315, 1229: 0.0007344752388809951, 1230: 0.0003223774348955371, 1231: 0.00036927621861152144, 1232: 0.0003338996324008741, 1233: 0.00036927621861152144, 1234: 0.00031540792929201224, 1235: 0.00035912816811717774, 1236: 0.00036927621861152144, 1237: 0.0003110306016705725, 1238: 0.0002715928412179222, 1239: 0.0003746040662198781, 1240: 0.00040805600830895185, 1241: 0.0002888711579954815, 1242: 0.00032509153116942876, 1243: 0.000354520876070631, 1244: 0.00031016818816943743, 1245: 0.00036137281814322146, 1246: 0.00029182666717309685, 1247: 0.000309897200581987, 1248: 0.0003043719106931658, 1249: 0.0003271969421007793, 1250: 0.0004116196366075381, 1251: 0.00032340689963900775, 1252: 0.00032008852920866094, 1253: 0.00029133299674543233, 1254: 0.00030877545147597917, 1255: 0.0003489237069874974, 1256: 0.00046920950550211564, 1257: 0.0007357796692574561, 1258: 0.0003080026010252578, 1259: 0.00036201086464548283, 1260: 0.0003450431330026833, 1261: 0.0002863327266214203, 1262: 0.00042427558585230373, 1263: 0.00036927621861152144, 1264: 0.0003456256533522977, 1265: 0.0003083504313293077, 1266: 0.00031684820946124354, 1267: 0.000305162781340299, 1268: 0.0003197986488561557, 1269: 0.0005683517956939294, 1270: 0.00029359971459650314, 1271: 0.00037738274767749197, 1272: 0.00032259162698785194, 1273: 0.00042889962844453236, 1274: 0.0002810747230047875, 1275: 0.0003146354312990401, 1276: 0.00033809227420157834, 1277: 0.00030832355294157945, 1278: 0.000321536520707037, 1279: 0.00034693601821261685, 1280: 0.0003390883715976621, 1281: 0.00039724179570827645, 1282: 0.0004534913638402451, 1283: 0.0003015634652317214, 1284: 0.00042128266052487827, 1285: 0.00028419291494189217, 1286: 0.00036927621861152144, 1287: 0.0003701596216980181, 1288: 0.0002698844957823892, 1289: 0.0003054042783496733, 1290: 0.00045500890085569995, 1291: 0.0003214134498053751, 1292: 0.00028958798230724407, 1293: 0.00029837248703444607, 1294: 0.00029618843664932896, 1295: 0.0002966102496478758, 1296: 0.0004953108827566912, 1297: 0.00029809089886357357, 1298: 0.00036927621861152144, 1299: 0.00028452765518614467, 1300: 0.000450241572022962, 1301: 0.0004112727555055363, 1302: 0.0003045454282442001, 1303: 0.0003824042588006315, 1304: 0.0003356903007875453, 1305: 0.0002808915300041736, 1306: 0.0002778624381677887, 1307: 0.0005249944423557125, 1308: 0.0003184226017309663, 1309: 0.0007180692681713383, 1310: 0.00036927621861152144, 1311: 0.0003585650746270906, 1312: 0.00032182020389079455, 1313: 0.00040696637206740535, 1314: 0.0003438447224275847, 1315: 0.00029748176928524715, 1316: 0.000374446556153927, 1317: 0.0005575697728998491, 1318: 0.00037713311913793, 1319: 0.0002907525843961743, 1320: 0.00042452434378111145, 1321: 0.00029529701144150966, 1322: 0.00038123878762290857, 1323: 0.00027650981013928024, 1324: 0.0003300949845769522, 1325: 0.0004207456722836926, 1326: 0.00031529984848442666, 1327: 0.0003723993638826812, 1328: 0.00035162231995680265, 1329: 0.0003359798688816854, 1330: 0.0003424891250062799, 1331: 0.0003174520623926579, 1332: 0.00031179445888162633, 1333: 0.0003800837988438402, 1334: 0.0003340298861559543, 1335: 0.0003372323677538364, 1336: 0.0004895912431982969, 1337: 0.0002980564444096603, 1338: 0.0002999578402442302, 1339: 0.0003017549570895854, 1340: 0.00034345785969019753, 1341: 0.0003082367294623725, 1342: 0.0003223774348955371, 1343: 0.0004678818849411408, 1344: 0.00043587491391602906, 1345: 0.0003169970969224872, 1346: 0.00029744289625670446, 1347: 0.00030893228437129565, 1348: 0.00037555456427392696, 1349: 0.0003827939716436758, 1350: 0.00028328357009251796, 1351: 0.0005780000456333445, 1352: 0.0002979602363529165, 1353: 0.00031547294054302396, 1354: 0.0003862775600880882, 1355: 0.0002922755267295163, 1356: 0.00036927621861152144, 1357: 0.0003291084879148738, 1358: 0.006028583623399497, 1359: 0.00039360159977844185, 1360: 0.000478649866298124, 1361: 0.00029457469247993594, 1362: 0.00032958257059084966, 1363: 0.00027165494111814573, 1364: 0.0003109493078623346, 1365: 0.00033126836604306144, 1366: 0.0003222680122150653, 1367: 0.0005044083558897556, 1368: 0.0003232602881601411, 1369: 0.00031213250865857, 1370: 0.0004249395144356832, 1371: 0.00036927621861152144, 1372: 0.0003401496344422429, 1373: 0.0003465309851171267, 1374: 0.00028093062779045925, 1375: 0.00036927621861152144, 1376: 0.0003173837931901978, 1377: 0.00041302205347099676, 1378: 0.0004300394248427917, 1379: 0.00031747008989584953, 1380: 0.00034773402611602117, 1381: 0.00032088187689286005, 1382: 0.00037856162354369505, 1383: 0.00032391591313930736, 1384: 0.00027044503154502274, 1385: 0.00031548901944235665, 1386: 0.0004161005374883522, 1387: 0.0003161944350383594, 1388: 0.00036756523559244, 1389: 0.00029794124074573056, 1390: 0.0003464071497264842, 1391: 0.0002979405550445859, 1392: 0.00033251325458478456, 1393: 0.00036927621861152144, 1394: 0.00031768462080080615, 1395: 0.0004841112235322359, 1396: 0.0003771650413409134, 1397: 0.00030013196556239613, 1398: 0.00029457469247993594, 1399: 0.000356314557955061, 1400: 0.0003609446326724685, 1401: 0.00040951302194108265, 1402: 0.00038907402081021, 1403: 0.000325033684413016, 1404: 0.0002985072765233327, 1405: 0.0003736086495806851, 1406: 0.0003271211549272393, 1407: 0.00036927621861152144, 1408: 0.00038177113145393887, 1409: 0.0004030633551951264, 1410: 0.0003207606320730627, 1411: 0.0003323845652369494, 1412: 0.0003144454221863176, 1413: 0.0008175235381962129, 1414: 0.00033988598296981823, 1415: 0.0003668557741482138, 1416: 0.0008533459461941185, 1417: 0.0004447207198899655, 1418: 0.00031264266178186067, 1419: 0.00036121522047747115, 1420: 0.00031603026919306576, 1421: 0.0004192762797848414, 1422: 0.0003094799497131733, 1423: 0.00029820559789114497, 1424: 0.00035403376556642657, 1425: 0.00028784185020800756, 1426: 0.0003177838106547965, 1427: 0.00032452180516634433, 1428: 0.0004617611886294704, 1429: 0.00033004806327789307, 1430: 0.00036343678318121955, 1431: 0.0003253056900928219, 1432: 0.0003686844388083811, 1433: 0.0003574110622968325, 1434: 0.0003235365265660952, 1435: 0.00030723967897684806, 1436: 0.00030698336081980983, 1437: 0.0003093070907806884, 1438: 0.00036927621861152144, 1439: 0.00036927621861152144, 1440: 0.0002854088700601036, 1441: 0.0014211611868052035, 1442: 0.0003967907042500198, 1443: 0.0004997752514480884, 1444: 0.00035588662982696974, 1445: 0.00033440662453186246, 1446: 0.00033440662453186246, 1447: 0.00029684808263909186, 1448: 0.0003269249883546049, 1449: 0.000313641088273758, 1450: 0.00037246643111230954, 1451: 0.0003112088983519282, 1452: 0.00030194969664243897, 1453: 0.0002734473253889376, 1454: 0.0002939984753068768, 1455: 0.0003055432843763868, 1456: 0.00032246379177745625, 1457: 0.00045694082692982347, 1458: 0.00029219919656059407, 1459: 0.0003267505653243365, 1460: 0.0003199801311161432, 1461: 0.00029163052193935233, 1462: 0.00037367580304164096, 1463: 0.0004957043550591474, 1464: 0.0005145792767641921, 1465: 0.0003087907723636226, 1466: 0.00028793235981125167, 1467: 0.00029736897581183076, 1468: 0.0003319809768338898, 1469: 0.0004317828281899761, 1470: 0.00041689687854419186, 1471: 0.0003037533902639529, 1472: 0.00035281469506068984, 1473: 0.0003474004784168006, 1474: 0.00036111110369992214, 1475: 0.00047599780620015294, 1476: 0.00036927621861152144, 1477: 0.0003569256694722123, 1478: 0.00032273090277606736, 1479: 0.0003252708170939795, 1480: 0.00027101352587860034, 1481: 0.00036641907385718316, 1482: 0.000370826423379283, 1483: 0.0008363189102812573, 1484: 0.0003608165247619829, 1485: 0.00027387610335927615, 1486: 0.00030258852102675257, 1487: 0.00034989955786248687, 1488: 0.0004014768546473952, 1489: 0.0003229112581667339, 1490: 0.00030860587879222724, 1491: 0.00030289092100263743, 1492: 0.0002697748371050442, 1493: 0.0002784896402966496, 1494: 0.00028910223219060725, 1495: 0.0003269901887360734, 1496: 0.00036927621861152144, 1497: 0.0002912345919801897, 1498: 0.00033799348660535185, 1499: 0.00031436368794968036, 1500: 0.00032557396623108994, 1501: 0.0004647503373106299, 1502: 0.00036699663469809223, 1503: 0.00035876528186716376, 1504: 0.0003250911190069462, 1505: 0.0006292950955509879, 1506: 0.0002969203845182574, 1507: 0.00041454658285458904, 1508: 0.00031278546104878676, 1509: 0.0003010072965692165, 1510: 0.0003784070306562335, 1511: 0.0002923800545403501, 1512: 0.00038128915427916707, 1513: 0.000322738117632456, 1514: 0.0002781447469016584, 1515: 0.00034516389039556474, 1516: 0.00031350770959222854, 1517: 0.00027647331179630823, 1518: 0.0003252989966694884, 1519: 0.0005526466367025588, 1520: 0.0003509023931176104, 1521: 0.0003420526085390405, 1522: 0.0003551145349095762, 1523: 0.00036651202871161706, 1524: 0.0003563005923497354, 1525: 0.0003011766178104001, 1526: 0.0004497114472831439, 1527: 0.0005005860697514693, 1528: 0.00043116537836333953, 1529: 0.00043712395587650207, 1530: 0.0003568741962074307, 1531: 0.00037275526893206445, 1532: 0.00031555023837802663, 1533: 0.0003508864537407323, 1534: 0.0004049916081745727, 1535: 0.0004072121316682013, 1536: 0.0003935153531509681, 1537: 0.00033910173008351477, 1538: 0.0004439245656621826, 1539: 0.00027387610335927615, 1540: 0.0002814582948333587, 1541: 0.00036927621861152144, 1542: 0.0012041377671322948, 1543: 0.00035122232642192746, 1544: 0.0003178715434136053, 1545: 0.00028713531006328917, 1546: 0.0003461843320032994, 1547: 0.0002971817558937444, 1548: 0.0003131939579100989, 1549: 0.00033337805777521235, 1550: 0.0002882033133219579, 1551: 0.0002722146812738913, 1552: 0.00038104447387939865, 1553: 0.0003490247526741182, 1554: 0.00043213180891019446, 1555: 0.00038803345220354347, 1556: 0.0003128299202786444, 1557: 0.00032191571054870775, 1558: 0.0005722701918161645, 1559: 0.0003596681203613482, 1560: 0.00029793922064895185, 1561: 0.00027450700546618724, 1562: 0.0002697748371050442, 1563: 0.00036927621861152144, 1564: 0.0002967204171922134, 1565: 0.0002762214891724987, 1566: 0.0003903972922267875, 1567: 0.000356485150788155, 1568: 0.0003118784871696742, 1569: 0.0003343742167662113, 1570: 0.000345176545833181, 1571: 0.00028763311098058857, 1572: 0.00029664301569062455, 1573: 0.00031757962708729216, 1574: 0.00034501257869250067, 1575: 0.0003035270563994463, 1576: 0.0002947352988397538, 1577: 0.00045445067971056037, 1578: 0.0003130129569941832, 1579: 0.00042590811455611016, 1580: 0.00037525744855745083, 1581: 0.00030749683612915884, 1582: 0.0003194112795274946, 1583: 0.00048272163802401234, 1584: 0.0002830079553750391, 1585: 0.0003152834986112277, 1586: 0.00030214534631087367, 1587: 0.000289258352585706, 1588: 0.00034430012267005615, 1589: 0.0003234418270346327, 1590: 0.00029700754410990726, 1591: 0.0003289081300680281, 1592: 0.0003401677325702742, 1593: 0.00039923819288478773, 1594: 0.00036927621861152144, 1595: 0.0003023253882238037, 1596: 0.0002854303080872025, 1597: 0.0003656355643086182, 1598: 0.00035496654617607265, 1599: 0.00043647190148188303, 1600: 0.00036927621861152144, 1601: 0.000286546575771131, 1602: 0.0005202519980901033, 1603: 0.0003037937327905624, 1604: 0.00035521790210851485, 1605: 0.00033150131698109865, 1606: 0.0003153088327985968, 1607: 0.0003196757526446809, 1608: 0.0004051585886076784, 1609: 0.00032771556886799716, 1610: 0.00048485973442831046, 1611: 0.0002992997236611885, 1612: 0.00029795913700279624, 1613: 0.00036927621861152144, 1614: 0.0003001005482650689, 1615: 0.00029525865322762353, 1616: 0.0005152553839208444, 1617: 0.00028093062779045925, 1618: 0.0003679578845135334, 1619: 0.00031053744765551834, 1620: 0.00036962514022580926, 1621: 0.00034330504190454767, 1622: 0.0003919931665583979, 1623: 0.0015548431677556125, 1624: 0.0007344489344154946, 1625: 0.0002898672262278896, 1626: 0.0002821936821621061, 1627: 0.00032474151113560945, 1628: 0.0007422199588470749, 1629: 0.000363411439269509, 1630: 0.00031216911113262095, 1631: 0.00031222325897696496, 1632: 0.00031053690123434045, 1633: 0.0002941909987895086, 1634: 0.00046795748117753485, 1635: 0.00033557121174796167, 1636: 0.00035725176648691425, 1637: 0.00028663520773346973, 1638: 0.0002915738278160958, 1639: 0.0003503392241421781, 1640: 0.0002832546036174837, 1641: 0.00035877312915299435, 1642: 0.0004550787730593295, 1643: 0.00030346553109272746, 1644: 0.0002707545114178676, 1645: 0.00035358778689528855, 1646: 0.0003037533902639529, 1647: 0.000401138637244587, 1648: 0.00036927621861152144, 1649: 0.00028387531029371575, 1650: 0.0003965589145343211, 1651: 0.0002921961555505119, 1652: 0.0003750080988877724, 1653: 0.0003765308059398688, 1654: 0.00033624022391852475, 1655: 0.0006003952321984001, 1656: 0.0002830079553750391, 1657: 0.00037713311913793, 1658: 0.0002801752930130475, 1659: 0.0003318767809890864, 1660: 0.0003214134498053751, 1661: 0.0003021717900472304, 1662: 0.00037239752444371687, 1663: 0.0003405200130987095, 1664: 0.0004285679413733568, 1665: 0.00036700957269518995, 1666: 0.0004377339976969792, 1667: 0.0003136893077598063, 1668: 0.00035260853457708027, 1669: 0.0003572299751475232, 1670: 0.00035521843585367344, 1671: 0.000423266010267634, 1672: 0.00029540413845262684, 1673: 0.00036927621861152144, 1674: 0.0003355484198985002, 1675: 0.0003272106428939502, 1676: 0.00036108797644615566, 1677: 0.0003271430762740358, 1678: 0.0003178627797449296, 1679: 0.00038112867173473907, 1680: 0.00030943717514902264, 1681: 0.0005230857819001106, 1682: 0.0005391946505697101, 1683: 0.0003364034063086808, 1684: 0.00037672560363442625, 1685: 0.0003003002674019466, 1686: 0.0003118571669811614, 1687: 0.0003976142174915913, 1688: 0.00037760962514065267, 1689: 0.00028754400589412583, 1690: 0.0002845207168823677, 1691: 0.00036927621861152144, 1692: 0.0009570711313366732, 1693: 0.00046466654462128706, 1694: 0.00032194198200598685, 1695: 0.0003241629949014014, 1696: 0.00029539342404517665, 1697: 0.0003859694265889878, 1698: 0.0003011842491052437, 1699: 0.0003036691131803939, 1700: 0.00028573652589228147, 1701: 0.0034104345907064516, 1702: 0.0005081054256133321, 1703: 0.0007770305859283161, 1704: 0.0003270760645962879, 1705: 0.0004891861377925128, 1706: 0.0003282293706815634, 1707: 0.00028573652589228147, 1708: 0.0004452878363256603, 1709: 0.000333230838463001, 1710: 0.0003452051911295588, 1711: 0.000378321471436792, 1712: 0.0002851087241407844, 1713: 0.0003681484816666586, 1714: 0.0003180183704892139, 1715: 0.000345248130746671, 1716: 0.00031833103388603887, 1717: 0.0003751856646723936, 1718: 0.0003622742807660458, 1719: 0.00029752639468760367, 1720: 0.00029752639468760367, 1721: 0.0004371007085959493, 1722: 0.0003187367307556845, 1723: 0.0003479519538739025, 1724: 0.00036628036336403626, 1725: 0.0007115400971521075, 1726: 0.0002829257271677329, 1727: 0.00032369679232466134, 1728: 0.0004309544320381313, 1729: 0.0003406127516040917, 1730: 0.0002807543395070188, 1731: 0.0003574138430559863, 1732: 0.00032685331176197414, 1733: 0.000290469352544554, 1734: 0.0003013772554283951, 1735: 0.0004043107967883373, 1736: 0.00031995095652435034, 1737: 0.00042945609378496963, 1738: 0.0003654946157116819, 1739: 0.0004864497663873742, 1740: 0.0006434442868185421, 1741: 0.000351208000183945, 1742: 0.0006903464555022442, 1743: 0.0004198722971791263, 1744: 0.0002697748371050442, 1745: 0.00041009012233916354, 1746: 0.0003373802355051377, 1747: 0.0002848401739794982, 1748: 0.0003035519490438461, 1749: 0.0003033787560360497, 1750: 0.0004228241245192812, 1751: 0.0003306755351085562, 1752: 0.000296797573983627, 1753: 0.0003017549570895854, 1754: 0.0003338942012169918, 1755: 0.0002909261398823193, 1756: 0.0003917815117127464, 1757: 0.00029283107217311917, 1758: 0.0003838777489288155, 1759: 0.00037487072864476163, 1760: 0.00027101352587860034, 1761: 0.000420118710683492, 1762: 0.00033556325538534095, 1763: 0.0003482259755576649, 1764: 0.00028453684520936083, 1765: 0.0008069966011074813, 1766: 0.000293651042917959, 1767: 0.0003041579089120798, 1768: 0.0003360360585545462, 1769: 0.0003038419288125246, 1770: 0.00030216996155157873, 1771: 0.000278488295673448, 1772: 0.0003857103455329082, 1773: 0.0003645867074422511, 1774: 0.00027029351361800567, 1775: 0.00031941656488968347, 1776: 0.0005635781326194836, 1777: 0.0003271786607889032, 1778: 0.0003981486447040083, 1779: 0.0003526412161519791, 1780: 0.0003025929996821563, 1781: 0.00032887069399379986, 1782: 0.000382394069172079, 1783: 0.0003041579089120798, 1784: 0.00031655981585708914, 1785: 0.000352506491853484, 1786: 0.00032931137370976083, 1787: 0.0003328947801485682, 1788: 0.00028805004771669635, 1789: 0.00033151876870290736, 1790: 0.00031552994790471256, 1791: 0.0003548927469374249, 1792: 0.00028897413827685514, 1793: 0.00030357161632964225, 1794: 0.0003978373989609264, 1795: 0.00045978340962831837, 1796: 0.0003530506261574582, 1797: 0.00034945592707125194, 1798: 0.0004319791144466557, 1799: 0.00032517378524587863, 1800: 0.00036069897392199113, 1801: 0.00034354732832514657, 1802: 0.0002933329468589773, 1803: 0.00038739841737836164, 1804: 0.0003220676190314961, 1805: 0.00033003667771322324, 1806: 0.00033657086793670223, 1807: 0.000337318513041295, 1808: 0.00031783873567654277, 1809: 0.00031575252266518976, 1810: 0.0019188873006146744, 1811: 0.0003089542081454803, 1812: 0.00037625931805469114, 1813: 0.0003180330419373147, 1814: 0.0002778244063504221, 1815: 0.00028725545356150426, 1816: 0.00034418420370649854, 1817: 0.00027269005304367266, 1818: 0.00032781985167974054, 1819: 0.0003234330669848233, 1820: 0.00029480198562568526, 1821: 0.000344458391814599, 1822: 0.0003089542081454803, 1823: 0.000345563546492563, 1824: 0.00047233511209841675, 1825: 0.0003307828461043634, 1826: 0.0004607792785040563, 1827: 0.00035418780551294514, 1828: 0.00040055508891709014, 1829: 0.0003987322650325299, 1830: 0.0003451995540797294, 1831: 0.0003689757241995475, 1832: 0.0003003933099293573, 1833: 0.0003139672746813248, 1834: 0.0003977361861268609, 1835: 0.00033875321910753936, 1836: 0.0003389767382296708, 1837: 0.0004034328459425423, 1838: 0.000338603876781345, 1839: 0.0004565325889706818, 1840: 0.0003043119904141928, 1841: 0.00033081845033504874, 1842: 0.0004726571681734014, 1843: 0.0002707545114178676, 1844: 0.0003128981959595285, 1845: 0.0002950555003743437, 1846: 0.0003546832537677731, 1847: 0.0002798201539327991, 1848: 0.00043644873301926457, 1849: 0.00042444277065768266, 1850: 0.0003248534648050582, 1851: 0.00036495601481830305, 1852: 0.0002690672939257301, 1853: 0.00027650981013928024, 1854: 0.00027650981013928024, 1855: 0.0002812913387334255, 1856: 0.00034126357332047667, 1857: 0.0003160884898906734, 1858: 0.0002934763407688468, 1859: 0.0002734473253889376, 1860: 0.00027650981013928024, 1861: 0.00027650981013928024, 1862: 0.00041541868466366054, 1863: 0.00031111721337158946, 1864: 0.0003189055923591158, 1865: 0.0003238679781142973, 1866: 0.0003007153862868868, 1867: 0.0003032291825135616, 1868: 0.00032247204367509227, 1869: 0.0005618398328705401, 1870: 0.0003129615524216768, 1871: 0.0003298487058595853, 1872: 0.00027650981013928024, 1873: 0.0003524179390402857, 1874: 0.0004241771507488349, 1875: 0.0002836773525592168, 1876: 0.0002734473253889376, 1877: 0.00031642965932550805, 1878: 0.00032766806817689304, 1879: 0.0003383106263142015, 1880: 0.00038200791949645724, 1881: 0.00036313665867851073, 1882: 0.00037398499076128915, 1883: 0.0002803926881588954, 1884: 0.00042107351591095756, 1885: 0.0003624263949417533, 1886: 0.00027878937098738577, 1887: 0.00029916337856666903, 1888: 0.00032986030542480303, 1889: 0.0005422594163439014, 1890: 0.00029018440553940905, 1891: 0.00030011898198021176, 1892: 0.00031342888439311856, 1893: 0.0003172835063723995, 1894: 0.0006972600212478157, 1895: 0.00029025450663996673, 1896: 0.0003911950229889031, 1897: 0.0003049145535792859, 1898: 0.00035752514289515396, 1899: 0.00042561526203364463, 1900: 0.0003061617739255332, 1901: 0.00043528502933561704, 1902: 0.0005555467725180333, 1903: 0.0005456648719027609, 1904: 0.0003109069636415718, 1905: 0.0002788046564849387, 1906: 0.00035788441075253034, 1907: 0.0005692669157750451, 1908: 0.0004083791531572589, 1909: 0.0003742749539497232, 1910: 0.00029094644560280535, 1911: 0.00035317525391677905, 1912: 0.0005978292311315679, 1913: 0.0003546475274454563, 1914: 0.0013293438502778808, 1915: 0.00031393044971727343, 1916: 0.00032549592441394424, 1917: 0.0003133105160897221, 1918: 0.00029297624123744427, 1919: 0.00035553837121974426, 1920: 0.00045756593182345114, 1921: 0.0003163301030526588, 1922: 0.0002967890562295619, 1923: 0.00028017664659017467, 1924: 0.00032450909615350915, 1925: 0.0003491898686135497, 1926: 0.00041122685063265497, 1927: 0.0007402996518765211, 1928: 0.00045452095570141084, 1929: 0.00033555121944445546, 1930: 0.0002756844301519516, 1931: 0.0002808774522081916, 1932: 0.0002949626460137434, 1933: 0.0003032004868210674, 1934: 0.00029296006719197127, 1935: 0.000343481392830379, 1936: 0.00033567131986222574, 1937: 0.00036651202871161706, 1938: 0.0003277801399345638, 1939: 0.00034180884128885616, 1940: 0.0003032339844191379, 1941: 0.0003407216536128149, 1942: 0.0003184308665641314, 1943: 0.00028606860789977804, 1944: 0.0004072136376067171, 1945: 0.0003927173164506686, 1946: 0.00031908159601048416, 1947: 0.00032259162698785194, 1948: 0.0003091268283787812, 1949: 0.00029114983295220855, 1950: 0.0007277309822033226, 1951: 0.0003722049311450801, 1952: 0.000595720879096405, 1953: 0.0003321887041734774, 1954: 0.0005232014864952688, 1955: 0.0005453598027375631, 1956: 0.0003478609684637717, 1957: 0.00044604921442555984, 1958: 0.0004270719383868164, 1959: 0.00041443750448664357, 1960: 0.00035630542837984804, 1961: 0.0003411239449315468, 1962: 0.0003523047071425857, 1963: 0.00031002533851164294, 1964: 0.0003979377366138199, 1965: 0.00029840265796972414, 1966: 0.0006745678915634199, 1967: 0.00031311588150860335, 1968: 0.00029390743463956494, 1969: 0.0003054189317279832, 1970: 0.00030168958943313355, 1971: 0.0002916276783549164, 1972: 0.00031371812649183013, 1973: 0.0007283026201310901, 1974: 0.0003138775518816821, 1975: 0.0003911912725279165, 1976: 0.00034696038743226617, 1977: 0.0002829270940664315, 1978: 0.00038941026202234157, 1979: 0.0003348510826653696, 1980: 0.0003352070788848034, 1981: 0.00029794008843105464, 1982: 0.0002984338821545707, 1983: 0.00031831533991567764, 1984: 0.0003758871156616572, 1985: 0.00030051757253568177, 1986: 0.0027170815835937715, 1987: 0.00035484068212230497, 1988: 0.00031656282773549847, 1989: 0.0003034573126986206, 1990: 0.0002736062043755498, 1991: 0.0003074365985424839, 1992: 0.00028783792818275807, 1993: 0.0002891467907709798, 1994: 0.0003123096351093333, 1995: 0.0003046091646218697, 1996: 0.00039880873075674484, 1997: 0.00034479821953899717, 1998: 0.00042991628550628644, 1999: 0.0003841454428807868, 2000: 0.00036660255142143873, 2001: 0.0007736700494597495, 2002: 0.00030574280151218953, 2003: 0.00036932433260691746, 2004: 0.0003239637740784667, 2005: 0.0002736062043755498, 2006: 0.0003704133357499222, 2007: 0.0002736062043755498, 2008: 0.0003149259957661877, 2009: 0.0003294939864326599, 2010: 0.00034072910118810966, 2011: 0.00029930889894461436, 2012: 0.00034155644830294925, 2013: 0.00029221903226517494, 2014: 0.0002702976031189485, 2015: 0.00028862210655025006, 2016: 0.00035556668947937407, 2017: 0.0004003224338031823, 2018: 0.0004744960614251445, 2019: 0.000285126104266461, 2020: 0.00028852570981862736, 2021: 0.00033991634818069923, 2022: 0.00033399188521334344, 2023: 0.00032692493617370066, 2024: 0.0004048259411926799, 2025: 0.0006503122571858965, 2026: 0.00030745495901802913, 2027: 0.0003789896971413858, 2028: 0.00027952211215249696, 2029: 0.00032305696280479873, 2030: 0.0003230715495136613, 2031: 0.00032089386759155556, 2032: 0.00028251195604749155, 2033: 0.00035006839616239687, 2034: 0.0017087168451196274, 2035: 0.00032201619191375155, 2036: 0.0002921818710762401, 2037: 0.00027732671310814853, 2038: 0.0003435650851739774, 2039: 0.0003219308002927595, 2040: 0.0003217721363619645, 2041: 0.0003211230359175311, 2042: 0.00029480118471299265, 2043: 0.0003196303221711696, 2044: 0.0002778660145312226, 2045: 0.0011185654229029433, 2046: 0.0003267847484451272, 2047: 0.0003022178105836937, 2048: 0.0002798276562896997, 2049: 0.0003097789710407408, 2050: 0.0002885351034139144, 2051: 0.00031559953812505364, 2052: 0.000311197622182916, 2053: 0.0002860966948050315, 2054: 0.00042622314235622337, 2055: 0.0003326695720350421, 2056: 0.0005042809177711919, 2057: 0.0002700105360036533, 2058: 0.0004300394248427917, 2059: 0.0004224832564733893, 2060: 0.0002868585478895006, 2061: 0.0002868585478895006, 2062: 0.0003043240044458469, 2063: 0.00041277290586175507, 2064: 0.00030248754573757063, 2065: 0.0003483504228719923, 2066: 0.00036204161859238024, 2067: 0.00034486683343419393, 2068: 0.0003454258107930058, 2069: 0.0003340966453677025, 2070: 0.0003340966453677025, 2071: 0.0004982242023774317, 2072: 0.0003027297386796359, 2073: 0.0002955272061629905, 2074: 0.00032373544315734466, 2075: 0.0004581240190461977, 2076: 0.00034522440453346146, 2077: 0.00039158812826686176, 2078: 0.00029664301569062455, 2079: 0.0003055652178686882, 2080: 0.0005081331502325061, 2081: 0.00029067071217284645, 2082: 0.0002890320553793495, 2083: 0.0005224299091816035, 2084: 0.0002910804991436454, 2085: 0.0003497534242194355, 2086: 0.0003205078109921555, 2087: 0.0002915295941642318, 2088: 0.000275832004560752, 2089: 0.00028626750871153637, 2090: 0.00033322595832798607, 2091: 0.0002872142640152266, 2092: 0.0002685517392111528, 2093: 0.00030897222248603634, 2094: 0.00040048775485201384, 2095: 0.0003823697100482014, 2096: 0.0004249866296999128, 2097: 0.0005283718887461829, 2098: 0.0003167302236301681, 2099: 0.0003370062235341182, 2100: 0.0004381961234007452, 2101: 0.0003119500836687816, 2102: 0.0002999578402442302, 2103: 0.0002999578402442302, 2104: 0.0003257190530955028, 2105: 0.00032244292397572757, 2106: 0.0002876555078293009, 2107: 0.00032330978833022573, 2108: 0.0003026516692633224, 2109: 0.0003837732870265, 2110: 0.0003282673440173497, 2111: 0.0002864674959117747, 2112: 0.0004262040960604425, 2113: 0.0003511169029573602, 2114: 0.0004477296015968892, 2115: 0.00033300493380648653, 2116: 0.0003067960798330843, 2117: 0.0005126053886026382, 2118: 0.00031278546104878676, 2119: 0.00028281515576538763, 2120: 0.0004099695192881734, 2121: 0.00029410922206469923, 2122: 0.0005310328681622221, 2123: 0.0003941360055025345, 2124: 0.0004385646919656474, 2125: 0.00034185675314672904, 2126: 0.0004553998503814646, 2127: 0.00035272887911224, 2128: 0.0002906497454618752, 2129: 0.00034349571480885705, 2130: 0.00078117134355765, 2131: 0.0003030420543313429, 2132: 0.00036772390999194934, 2133: 0.0004403358930482163, 2134: 0.00030380644684124586, 2135: 0.0003453990968232687, 2136: 0.0003228983131908855, 2137: 0.0003308860806959603, 2138: 0.000366456876914791, 2139: 0.0003480243646403126, 2140: 0.0004055676737732247, 2141: 0.00038884290888868467, 2142: 0.0002981248429913702, 2143: 0.0003309861893798752, 2144: 0.00045258383922952886, 2145: 0.0003562585405746206, 2146: 0.0004790252296789724, 2147: 0.0004916758076596535, 2148: 0.00036570135992835027, 2149: 0.0002929676038338684, 2150: 0.0003505591565448131, 2151: 0.00032288146337975873, 2152: 0.0002804175988846966, 2153: 0.0002845424573164662, 2154: 0.00036963664487173805, 2155: 0.0007780396182453954, 2156: 0.00033634726002635925, 2157: 0.00046365870089241153, 2158: 0.0002977403119246961, 2159: 0.00038010995038193944, 2160: 0.0005731512994232106, 2161: 0.00031563060784961484, 2162: 0.0003672586236655937, 2163: 0.0003932811919582358, 2164: 0.00033655485185878124, 2165: 0.00029692915259276707, 2166: 0.0003100722251388996, 2167: 0.00028266076236324296, 2168: 0.0004854824088866241, 2169: 0.00045759803319286114, 2170: 0.0003873595540424883, 2171: 0.00033246400547168764, 2172: 0.0003598123309507872, 2173: 0.00042608817286534035, 2174: 0.000340870241484612, 2175: 0.00035722113227557104, 2176: 0.0004745221407533012, 2177: 0.000376075459202297, 2178: 0.0003903800847156085, 2179: 0.00028915997500277815, 2180: 0.00036455959545221296, 2181: 0.00044471927639409115, 2182: 0.0008818276816531703, 2183: 0.00028919041663186853, 2184: 0.00031840968028595487, 2185: 0.00032557396623108994, 2186: 0.0003489371586402663, 2187: 0.00032392440308607375, 2188: 0.0002794538864955617, 2189: 0.00040091666574904646, 2190: 0.00030159242432708437, 2191: 0.00030516285677210914, 2192: 0.00037334744338853005, 2193: 0.00030075304537363856, 2194: 0.0004898576549053778, 2195: 0.000418775788437555, 2196: 0.00031373205591854827, 2197: 0.00035935200147243454, 2198: 0.00034745168933705026, 2199: 0.0005342120188653568, 2200: 0.00029018695258338625, 2201: 0.00029608381899476783, 2202: 0.0003142710988569701, 2203: 0.0004339770046472757, 2204: 0.0003727742452678047, 2205: 0.00030981715050362735, 2206: 0.00031538021226970407, 2207: 0.0003378256327679805, 2208: 0.00027045632758591936, 2209: 0.0003693075719992719, 2210: 0.00032017514530732413, 2211: 0.0003040093565784721, 2212: 0.00036261471667607876, 2213: 0.00035018636443507483, 2214: 0.00033815940570996203, 2215: 0.0003491955320348333, 2216: 0.0004795814067048499, 2217: 0.0004318686923204894, 2218: 0.00038687772412862435, 2219: 0.0003861800951299737, 2220: 0.00036935252376523305, 2221: 0.0003948481654325874, 2222: 0.00032921582705249496, 2223: 0.0003297736730997692, 2224: 0.00046413275399814416, 2225: 0.00036213836760559, 2226: 0.0003066640200573439, 2227: 0.00043183982557332904, 2228: 0.0003868337343917406, 2229: 0.0003802359533766501, 2230: 0.00037865752977074465, 2231: 0.00031163915938569003, 2232: 0.0003175157364832768, 2233: 0.0003558929789129275, 2234: 0.0003231227481888358, 2235: 0.0003868791368882093, 2236: 0.000367624015645075, 2237: 0.0003140872099755796, 2238: 0.0005841354688426888, 2239: 0.0002786610257651344, 2240: 0.00040904452995352085, 2241: 0.000500100562170393, 2242: 0.00032805654919031027, 2243: 0.0004334424951504606, 2244: 0.00030050282692725683, 2245: 0.00031536576624324774, 2246: 0.0004200636396129305, 2247: 0.0004649347052700549, 2248: 0.0007504948071856181, 2249: 0.0003678230318099958, 2250: 0.0004290958531967539, 2251: 0.00032015598602447824, 2252: 0.00032015598602447824, 2253: 0.00029777940881592813, 2254: 0.00029942603469897415, 2255: 0.00036927621861152144, 2256: 0.0003020601556683305, 2257: 0.00029415201808882, 2258: 0.0003220871174529173, 2259: 0.00045471984094413206, 2260: 0.00034348070885131007, 2261: 0.00030349397844873024, 2262: 0.00028504789093491457, 2263: 0.00032946929627833746, 2264: 0.00031315222073463643, 2265: 0.00040117266052324416, 2266: 0.0004054114228064748, 2267: 0.0003118164624775028, 2268: 0.00040508651674355233, 2269: 0.0004377230856195476, 2270: 0.00042720432989014725, 2271: 0.00028573652589228147, 2272: 0.00028573652589228147, 2273: 0.00028573652589228147, 2274: 0.0003035504344199979, 2275: 0.0003204396224412548, 2276: 0.0003209509366912684, 2277: 0.0003215616788600228, 2278: 0.0003133260504172329, 2279: 0.0003224725081802485, 2280: 0.00034424145925208126, 2281: 0.00028605379512703293, 2282: 0.0003905275117666532, 2283: 0.00035592623499916944, 2284: 0.00030629892679036915, 2285: 0.0003949534953511014, 2286: 0.00033933177823223756, 2287: 0.00034070284170292895, 2288: 0.0003517693127369887, 2289: 0.0003306943579449626, 2290: 0.0004143541006798764, 2291: 0.0005634973874813732, 2292: 0.0003177834795145716, 2293: 0.00031722826660042264, 2294: 0.00030921129249372574, 2295: 0.00034799533834213483, 2296: 0.0003946451647266106, 2297: 0.0004131712470616483, 2298: 0.0003151975700137165, 2299: 0.00032298333598185713, 2300: 0.0002969079598722948, 2301: 0.00029439887738393295, 2302: 0.00027586124922723913, 2303: 0.00033866331201816587, 2304: 0.00035315397593630804, 2305: 0.0004340374492954846, 2306: 0.00044142719873058666, 2307: 0.00029713961197723557, 2308: 0.00043312301367808676, 2309: 0.0005511444629129243, 2310: 0.0003316823076351338, 2311: 0.00030943717514902264, 2312: 0.0003040552542538388, 2313: 0.0002953609277693925, 2314: 0.00030489830776125984, 2315: 0.00029472190642321433, 2316: 0.0002852342685073318, 2317: 0.0003427786502364278, 2318: 0.0002831491397114035, 2319: 0.0002963209128743857, 2320: 0.0002823245367281104, 2321: 0.0002996719003832178, 2322: 0.0002870949947385968, 2323: 0.00030114358583544333, 2324: 0.0003240085382142868, 2325: 0.00040763141980822495, 2326: 0.0003522253045933612, 2327: 0.00040114760759904706, 2328: 0.00029983795054397465, 2329: 0.00044628402754599773, 2330: 0.0003349755157590704, 2331: 0.000298629314995868, 2332: 0.00034388755354509645, 2333: 0.00033323964590987483, 2334: 0.0002885927247914978, 2335: 0.000515102449132168, 2336: 0.00037724386627596246, 2337: 0.00030698336081980983, 2338: 0.0003017479835236579, 2339: 0.0003880479189744235, 2340: 0.0003062780862737773, 2341: 0.00036927621861152144, 2342: 0.0003268741848583844, 2343: 0.00030240918230927186, 2344: 0.00037722318786694443, 2345: 0.0002781447469016584, 2346: 0.00030073805442607096, 2347: 0.0002908012698807123, 2348: 0.0003006511094886011, 2349: 0.00044223099958158547, 2350: 0.00031303889505092603, 2351: 0.00042417271620252443, 2352: 0.00032594959138569234, 2353: 0.00032490724687166204, 2354: 0.0002813909301809834, 2355: 0.000322000289701416, 2356: 0.00044446650469075227, 2357: 0.0003824584749933553, 2358: 0.00039649613954468037, 2359: 0.000577786645591145, 2360: 0.00038484313366173056, 2361: 0.0002929112563805934, 2362: 0.0002844847900771015, 2363: 0.00033494390224315273, 2364: 0.0003490376705325527, 2365: 0.0004447439519609612, 2366: 0.00036333356798251993, 2367: 0.000727361731685823, 2368: 0.0003038900308844069, 2369: 0.0003038900308844069, 2370: 0.0003545647185363544, 2371: 0.00031374881810676417, 2372: 0.00029208786101823594, 2373: 0.0003156895147037452, 2374: 0.0002913954009198604, 2375: 0.000391575953025625, 2376: 0.00046224811808953116, 2377: 0.00029931054330895756, 2378: 0.0003460732633676282, 2379: 0.00031731892796529724, 2380: 0.0003016280602747898, 2381: 0.0003152506872908256, 2382: 0.00030598708514815434, 2383: 0.0005422395219125749, 2384: 0.0002769868519048079, 2385: 0.0004745210308386019, 2386: 0.00033920114327681865, 2387: 0.00043809918624734573, 2388: 0.0004082731322411668, 2389: 0.00027905413982828235, 2390: 0.0003657524285654045, 2391: 0.000334495025834184, 2392: 0.0003300557802740109, 2393: 0.0002972696107311624, 2394: 0.0008357779286363799, 2395: 0.0003623457987103173, 2396: 0.00032887070838390865, 2397: 0.0002790320114582087, 2398: 0.0003168560006535506, 2399: 0.00034903433078414136, 2400: 0.00028119635491806536, 2401: 0.00036135024547937205, 2402: 0.0003504203848169526, 2403: 0.0003212750099781404, 2404: 0.000282467594527219, 2405: 0.0004089542506722093, 2406: 0.0003447224982123659, 2407: 0.0003267059154003828, 2408: 0.00031006246049350243, 2409: 0.0003630315825993221, 2410: 0.00036927621861152144, 2411: 0.00036927621861152144, 2412: 0.00035997115573630786, 2413: 0.00034474541884225236, 2414: 0.0003279601352284679, 2415: 0.00033089752524018893, 2416: 0.0003624568583242223, 2417: 0.00032695844424278613, 2418: 0.0004430771720115689, 2419: 0.00037195403609517456, 2420: 0.0002905818127773567, 2421: 0.0003044629024239349, 2422: 0.00032504658668469296, 2423: 0.0003589785581255537, 2424: 0.0003249130079258796, 2425: 0.0004341016662524486, 2426: 0.0002945410137977869, 2427: 0.0004123613710685214, 2428: 0.0003150131411192706, 2429: 0.00036927621861152144, 2430: 0.00031816363101555263, 2431: 0.00036927621861152144, 2432: 0.00036927621861152144, 2433: 0.00036927621861152144, 2434: 0.00038755721177360106, 2435: 0.00030834466353011703, 2436: 0.0003716414855031777, 2437: 0.000378970888059917, 2438: 0.0003352374684481585, 2439: 0.0003359248949248965, 2440: 0.0003168971087642512, 2441: 0.00028651235140968515, 2442: 0.00035402801660659543, 2443: 0.0003098537080097775, 2444: 0.0003827708349325226, 2445: 0.0003236857849319707, 2446: 0.0004712273990926005, 2447: 0.00031955607881811667, 2448: 0.0003774166033154074, 2449: 0.0003277801399345638, 2450: 0.0005228813477790271, 2451: 0.0006360817614047896, 2452: 0.0003008242217800379, 2453: 0.0003160299701362204, 2454: 0.0003270345417115176, 2455: 0.0003590786438575549, 2456: 0.00032547863836670556, 2457: 0.00043818081886394175, 2458: 0.0003428137192505971, 2459: 0.0003326249884350788, 2460: 0.00029885589137182954, 2461: 0.00036927621861152144, 2462: 0.00033443426417914307, 2463: 0.00036226872753097005, 2464: 0.000605459622784, 2465: 0.0003080946082411303, 2466: 0.0004456286052021092, 2467: 0.00035973146902273527, 2468: 0.00032636506857691537, 2469: 0.00034741344636658874, 2470: 0.0003253944780650921, 2471: 0.00030462177642217057, 2472: 0.0004902204189211675, 2473: 0.00033564759270516994, 2474: 0.0003252029517675363, 2475: 0.0003124148560317221, 2476: 0.00047002540095523386, 2477: 0.0003081010213892925, 2478: 0.0002889708578930056, 2479: 0.00036927621861152144, 2480: 0.00035606818455691437, 2481: 0.00032483181269706994, 2482: 0.00034258146396745506, 2483: 0.000340870241484612, 2484: 0.0003976821957384309, 2485: 0.0004212485525835859, 2486: 0.00037342036615832356, 2487: 0.0003178508862431723, 2488: 0.0003178508862431723, 2489: 0.0003003744280864313, 2490: 0.00030488132216012736, 2491: 0.00032262744353229924, 2492: 0.0003674556091474236, 2493: 0.0004216064409654273, 2494: 0.0003063362002598984, 2495: 0.00041265041573106245, 2496: 0.00029231632661550643, 2497: 0.0003999716649638426, 2498: 0.0003293934538001309, 2499: 0.0003201136519846827, 2500: 0.0003228625071970408, 2501: 0.00044094514665558697, 2502: 0.0003231227481888358, 2503: 0.00040224299901366186, 2504: 0.0003263434132686192, 2505: 0.00030288533464462963, 2506: 0.00031083202313088255, 2507: 0.00032826671197821945, 2508: 0.0002934822564498225, 2509: 0.0003885044213080693, 2510: 0.0002935690814024861, 2511: 0.0002945307960466501, 2512: 0.00032749750948228687, 2513: 0.00033005456220999284, 2514: 0.0002881894841425691, 2515: 0.0003840191484292836, 2516: 0.0003905737630525441, 2517: 0.00037956024482778644, 2518: 0.0004185582810681455, 2519: 0.0003656179369118215, 2520: 0.0003323752082368737, 2521: 0.0003084926672481289, 2522: 0.0003308973126327879, 2523: 0.00028026295907270416, 2524: 0.0003163201490576762, 2525: 0.0002871733387172964, 2526: 0.0004028466490272687, 2527: 0.0002970936073328444, 2528: 0.0003111788955230614, 2529: 0.00036927621861152144, 2530: 0.00040270737012233825, 2531: 0.0003040726817950982, 2532: 0.00030987340672748975, 2533: 0.0003114462574761674, 2534: 0.0004585543828265047, 2535: 0.00034565376042714645, 2536: 0.00030466495285528866, 2537: 0.00036927621861152144, 2538: 0.0003860649644498677, 2539: 0.0005301874253812952, 2540: 0.00031694045050226644, 2541: 0.0003712486629727038, 2542: 0.0002998380825021123, 2543: 0.00033341706000438343, 2544: 0.00036927621861152144, 2545: 0.0003272105285240303, 2546: 0.0003934528744175977, 2547: 0.0003213867211216983, 2548: 0.0003084384069855922, 2549: 0.0003084384069855922, 2550: 0.0002880633369174973, 2551: 0.0003025572748019058, 2552: 0.00029535292543118234, 2553: 0.0003772916824946262, 2554: 0.00033472008730245877, 2555: 0.0004205442285504051, 2556: 0.00033382826936842756, 2557: 0.00028668416711484827, 2558: 0.00033382826936842756, 2559: 0.0003040889803508439, 2560: 0.00040622602754238226, 2561: 0.0003140662188527643, 2562: 0.00042608817286534035, 2563: 0.0003223774348955371, 2564: 0.0004378282135372374, 2565: 0.0003976821957384309, 2566: 0.000340870241484612, 2567: 0.000340870241484612, 2568: 0.00030428399372272143, 2569: 0.00036927621861152144, 2570: 0.0005321756886212457, 2571: 0.00031556202550249776, 2572: 0.0003053277392497228, 2573: 0.00033811232991826025, 2574: 0.0003331700416309548, 2575: 0.00029995005887284253, 2576: 0.00036104775921421133, 2577: 0.0003158186284305893, 2578: 0.00031192200191258734, 2579: 0.00036046563389332414, 2580: 0.0003782412768870206, 2581: 0.00033475417222021334, 2582: 0.0003675196273248848, 2583: 0.00036927621861152144, 2584: 0.0002868181041762995, 2585: 0.0003282293706815634, 2586: 0.00036927621861152144, 2587: 0.0002931768079655593, 2588: 0.0002960430950310051, 2589: 0.00029251214859648444, 2590: 0.00032006710632439937, 2591: 0.000324815150928291, 2592: 0.0003019230768097089, 2593: 0.0002931406301167161, 2594: 0.000319614985997064, 2595: 0.00029698016135000797, 2596: 0.0003384666047990129, 2597: 0.00039384008582720007, 2598: 0.0003037009801865137, 2599: 0.0003395754922485944, 2600: 0.00036927621861152144, 2601: 0.0003368630972169857, 2602: 0.00036927621861152144, 2603: 0.00036927621861152144, 2604: 0.00037838054650804573, 2605: 0.0002963869510514176, 2606: 0.00033158905095190597, 2607: 0.0002994469114081603, 2608: 0.0003658601455583505, 2609: 0.00033506750909527265, 2610: 0.0003019434888321603, 2611: 0.00030876745877031036, 2612: 0.0003522806504020504, 2613: 0.00028749469787252464, 2614: 0.00047148995919190983, 2615: 0.0003596659342504993, 2616: 0.00031104310555010945, 2617: 0.00032676543441980885, 2618: 0.00036927621861152144, 2619: 0.00036927621861152144, 2620: 0.0003045701714167416, 2621: 0.00029230389950561685, 2622: 0.0002912233090786022, 2623: 0.00032424419524297306, 2624: 0.0003592722700456533, 2625: 0.00036927621861152144, 2626: 0.00036927621861152144, 2627: 0.00032194198200598685, 2628: 0.00031418574696524636, 2629: 0.00036927621861152144, 2630: 0.0003577484154666411, 2631: 0.00036927621861152144, 2632: 0.00031694045050226644, 2633: 0.00036927621861152144, 2634: 0.0004028466490272687, 2635: 0.00033570578819577416, 2636: 0.0004028466490272687, 2637: 0.00029185195558898325, 2638: 0.00029763633299176014, 2639: 0.00036927621861152144, 2640: 0.0002923800545403501, 2641: 0.0003286592745000717, 2642: 0.0003847347782306987, 2643: 0.00028638351778955144, 2644: 0.00030674862104680645, 2645: 0.0003529519386011072, 2646: 0.00029720484634963666, 2647: 0.0003935779414345944, 2648: 0.0003793429372328449, 2649: 0.00033703548378215796, 2650: 0.000310243739868459, 2651: 0.0003095446597555051, 2652: 0.0003198358952145315, 2653: 0.0003588317428194489, 2654: 0.0003852712592781183, 2655: 0.0003105844345517206, 2656: 0.00033539301646766465, 2657: 0.0003221700811506105, 2658: 0.0003221700811506105, 2659: 0.00036927621861152144, 2660: 0.00036927621861152144, 2661: 0.00032999152293577635, 2662: 0.00037713311913793, 2663: 0.0003156136394449725, 2664: 0.00036927621861152144, 2665: 0.00036927621861152144, 2666: 0.00036927621861152144, 2667: 0.0003878564388412469, 2668: 0.0003593822173243561, 2669: 0.00036927621861152144, 2670: 0.0003540885664305774, 2671: 0.0007356122218585939, 2672: 0.0002788985396553376, 2673: 0.00029786206132613945, 2674: 0.00032716150310046555, 2675: 0.00032716150310046555, 2676: 0.00036927621861152144, 2677: 0.00031155871577740065, 2678: 0.0002984796142777644, 2679: 0.00042082018493727445, 2680: 0.0003388858077835673, 2681: 0.0006067033197687852, 2682: 0.00031113314066610437, 2683: 0.000313641088273758, 2684: 0.00031848169392841724, 2685: 0.0003953446932095237, 2686: 0.00032999152293577635, 2687: 0.00032999152293577635, 2688: 0.00037713311913793, 2689: 0.00035523960734885527, 2690: 0.00036927621861152144, 2691: 0.00034579336115256624, 2692: 0.00036927621861152144, 2693: 0.00033570578819577416, 2694: 0.00036927621861152144, 2695: 0.00036927621861152144, 2696: 0.0003263383269433307, 2697: 0.00036927621861152144, 2698: 0.00035745930059413457, 2699: 0.0003258698796861838, 2700: 0.00032931461400662423, 2701: 0.0003592722700456533, 2702: 0.000313028125046347, 2703: 0.00036927621861152144, 2704: 0.00036927621861152144, 2705: 0.00036927621861152144, 2706: 0.0003820892345172981, 2707: 0.0003474004784168006}\n"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Convert DGL graph to NetworkX\n",
        "nx_graph = graph.to_networkx()\n",
        "\n",
        "# Run PageRank\n",
        "pagerank_scores = pagerank(nx_graph)\n",
        "\n",
        "# Assuming graph.ndata['feat'] or similar contains node features, map DGL nodes to NetworkX nodes\n",
        "# If the DGL graph uses integer indices for nodes, you can map directly\n",
        "\n",
        "# Print pagerank scores (optional)\n",
        "print(pagerank_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NnJt9F8zGAj",
        "outputId": "9205525e-d9b3-4952-b3fa-4016050529d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0003])\n",
            "SparseMatrix(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
            "                             [   0,  633, 1862,  ..., 1473, 2706, 2707]]),\n",
            "             values=tensor([0.0253, 0.0250, 0.0224,  ..., 0.0200, 0.0200, 0.0203]),\n",
            "             shape=(2708, 2708), nnz=13264)\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "\n",
        "features = graph.ndata[\"feat\"]\n",
        "adj = symmetric_normalize_adjacency(graph)\n",
        "# I_N = dglsp.identity((features.shape[0], features.shape[0]))\n",
        "# print(I_N)\n",
        "\n",
        "# Lazy random walk (also known as lazy graph convolution).\n",
        "# lazy_adj = dglsp.add((1 - args.beta) * I_N, args.beta * adj).to(args.device)\n",
        "# print(lazy_adj)\n",
        "\n",
        "# Convert PageRank scores to a tensor, matching DGL graph node indices with NetworkX nodes\n",
        "pagerank_tensor = th.tensor(\n",
        "    [pagerank_scores[int(node.item())] for node in graph.nodes()])\n",
        "\n",
        "# Normalize PageRank scores (optional)\n",
        "pagerank_tensor /= pagerank_tensor.sum()\n",
        "\n",
        "print(pagerank_tensor)\n",
        "\n",
        "# Assuming pagerank_tensor has been defined earlier and is on the correct device\n",
        "# Create a sparse diagonal matrix from the PageRank scores\n",
        "num_nodes = pagerank_tensor.shape[0]  # Get the number of nodes\n",
        "pagerank_diag = dglsp.diag(pagerank_tensor).to(\n",
        "    args.device)  # Convert to a DGL SparseMatrix\n",
        "\n",
        "# Now, modify lazy_adj by incorporating PageRank scores\n",
        "# Ensure adj is a DGL SparseMatrix as well\n",
        "lazy_adj = dglsp.add((1 - args.beta) * pagerank_diag, args.beta * adj)\n",
        "\n",
        "print(lazy_adj)  # Check the lazy adjacency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HZdqnLtuosuc"
      },
      "outputs": [],
      "source": [
        "model = OGC(graph).to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wo-0hIforph",
        "outputId": "cafd3b9c-b386-4867-c00b-807e5b1ddba0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10031"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZGP9yYeVz0Zt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1 loss_tv 0.1344 acc_tv 0.5797 acc_test 0.4440\n",
            "epoch 2 loss_tv 0.1363 acc_tv 0.5031 acc_test 0.5280\n",
            "epoch 3 loss_tv 0.1390 acc_tv 0.6313 acc_test 0.4690\n",
            "epoch 4 loss_tv 0.1386 acc_tv 0.6031 acc_test 0.5540\n",
            "epoch 5 loss_tv 0.1390 acc_tv 0.7156 acc_test 0.6310\n",
            "epoch 6 loss_tv 0.1391 acc_tv 0.7391 acc_test 0.6690\n",
            "epoch 7 loss_tv 0.1393 acc_tv 0.7797 acc_test 0.7110\n",
            "epoch 8 loss_tv 0.1395 acc_tv 0.7844 acc_test 0.7330\n",
            "epoch 9 loss_tv 0.1397 acc_tv 0.7937 acc_test 0.7370\n",
            "epoch 10 loss_tv 0.1399 acc_tv 0.7969 acc_test 0.7450\n",
            "epoch 11 loss_tv 0.1401 acc_tv 0.8016 acc_test 0.7450\n",
            "epoch 12 loss_tv 0.1403 acc_tv 0.8016 acc_test 0.7440\n",
            "epoch 13 loss_tv 0.1404 acc_tv 0.8016 acc_test 0.7400\n",
            "epoch 14 loss_tv 0.1406 acc_tv 0.8031 acc_test 0.7380\n",
            "Test Acc:0.7400\n",
            "Total Time:6.0121\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "res = train(model, features, lazy_adj, args)\n",
        "time_tot = time.time() - start_time\n",
        "\n",
        "print(f\"Test Acc:{res:.4f}\")\n",
        "print(f\"Total Time:{time_tot:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "056b821b52f549719e4b49583a1aad59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69f198b8b1c0460985761f8477fac1ef",
            "max": 238901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f74db49c87c14c26b6045c893d1bfb64",
            "value": 238901
          }
        },
        "18c282ed6e7348c1bdeebc7dc031278a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bb63e36125742e782aca29a37a4574e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83fd1cc79ffb4e358da3a4e2cdbf98dc",
              "IPY_MODEL_056b821b52f549719e4b49583a1aad59",
              "IPY_MODEL_62a93d8902634a0ca9f4dd35b1a7713a"
            ],
            "layout": "IPY_MODEL_18c282ed6e7348c1bdeebc7dc031278a"
          }
        },
        "62a93d8902634a0ca9f4dd35b1a7713a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1fd7c0036044bd096bbc87706177432",
            "placeholder": "​",
            "style": "IPY_MODEL_8c3caa59addd4042be54ce85c0849b5f",
            "value": " 239k/239k [00:00&lt;00:00, 3.37MB/s]"
          }
        },
        "69f198b8b1c0460985761f8477fac1ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83fd1cc79ffb4e358da3a4e2cdbf98dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2cd39f4ccda4efc8816a02337945955",
            "placeholder": "​",
            "style": "IPY_MODEL_e43d5bbcbbe04b9ba7862febb04088d3",
            "value": "/root/.dgl/citeseer.zip: 100%"
          }
        },
        "8c3caa59addd4042be54ce85c0849b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2cd39f4ccda4efc8816a02337945955": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1fd7c0036044bd096bbc87706177432": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43d5bbcbbe04b9ba7862febb04088d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f74db49c87c14c26b6045c893d1bfb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
